---
layout: post
title: The unification of representation learning and generative modelling
image: /assets/img/blog/r4g/overview_figure.png
accent_image: 
  background: url('/assets/img/blog/jj-ying.png') center/cover
  overlay: false
accent_color: '#ccc'
theme_color: '#ccc'
description: >
  A deep dive into the convergence of discriminative and generative AI, covering 4 phases of evolution from REPA to Pixel-Space Diffusion.
invert_sidebar: true
categories: ml
---

# The unification of representation learning and generative modelling

## Introduction

Both generative modeling and representation learning have made impressive advances in recent years, particularly in computer vision. Diffusion [^ho2020ddpm][^song2020score][^lai2025principles] and flow models [^lipman2024flow][^albergo2023building] have achieved unprecedented generation quality, while self-supervised paradigms like CLIP [^radford2021learning], DINO [^caron_emerging_2021], and MAE [^he2022masked] have enabled state-of-the-art performance on classification, detection, and depth estimation. Yet generation has remained separate from other vision tasks, raising a natural question: can we create unified representations useful for both discriminative and generative tasks?

![Evolution of representation learning and generative modeling (2019-2025)](/assets/img/blog/r4g/timeline.png)
*Fig 1. Three parallel timelines showing the independent evolution of representation learning methods, latent generative modeling architectures, and the recent convergence of these fields in R4G (Representation for Generation).*

This field—sometimes termed Representation for Generation (R4G)—has evolved rapidly over the past year, with multiple groups independently converging on similar insights. The rapid development reveals fundamental questions about visual representations: Are features learned during generation inherently different from those learned discriminatively? Can we bridge these paradigms for more efficient systems? Recent evidence suggests diffusion models already learn semantically meaningful representations [^kadkhodaie_unconditional_2025][^liang_how_2024], and that generative classifiers exhibit surprisingly human-like properties [^jaini_intriguing_2024]. Is there a way these two can benefit from each other more explicitly?

![Four phases of representation-guided generation](/assets/img/blog/r4g/overview_figure.png)
*Fig 2. Evolution from no alignment (Phase 0) through feature alignment (Phase 1), VAE alignment (Phase 2), VAE-less direct embedding diffusion (Phase 3), and pixel-space diffusion without pretrained models (Phase 4). Each block shows the architectural evolution and which papers introduced key innovations.*

When I started reading into the literature, I was honestly quite overwhelmed by the sheer number of papers and approaches being proposed there this year alone, with new papers coming out every week. But after some reading and discussion of some of these papers with the respective authors as well as colleagues some patterns started to emerge. In this blog post I try to organize recent developments into four phases reflecting my take on how the field developed during 2025: from initial alignment strategies to questioning whether pretrained representations are necessary at all. As part of this I also touch upon pixel-space versus latent diffusion models (again) and how the trend goes both ways, i.e. how we can use generative models for representation learning. Finally, because at heart I am a molecule guy, I share some of my thoughts on how these ideas are beginning to influence molecular machine learning, and exciting directions to pursue there.

* toc
{:toc}

## Background

To talk about the unification of representation learning and generative modelling, it might be wise to shortly talk about each of these separately and review what happened in each of them recently. It has been known for quite a while that they are intimitely related (for a recent take on this see [this excellent talk](https://www.youtube.com/watch?v=4VwXBrMoC0E) by Kaiming He from a CVPR2025 workshop). However, in practice they still function quite differently, both in terms of the losses and training recipes employed as well as the neural network architectures used. What is the latest in both of these fields?

![Generation vs Representation](/assets/img/blog/r4g/generation_vs_representation.png)
*Fig 3. Generative Modelling and Representation Learning are intimately connected, two sides of the same coin: while representation learning tries to map from data to some semantic representation space (e.g. to allow for easier classification of objects in an image), generative modelling wants to maps from abstract concepts like text prompts to actual data samples. Image from the ["Foundation of Computer Vision" online book](https://visionbook.mit.edu/generative_modeling_and_rep_learning.html)*

### Generative modelling: Latent Diffusion Models

![LDM training pipeline](/assets/img/blog/r4g/ldm_diagram.png)
*Fig 4. Two-stage training: first, a VAE compresses images into latent space; second, diffusion operates in this latent space. This modular design accelerated adoption but separated tokenizer training from diffusion model training.*

Diffusion models revolutionized generation by framing it as iterative denoising. Ho et al. [^ho2020ddpm] introduced DDPMs that gradually add noise and learn to reverse it; Song et al. looked at it from a score-based perspective; and in 2021 they got together to formalize a unified perspective through stochastic differential equations [^song2020score]. This was all still in pixel-space; each pixel was denoised in RGB space, hindering both scalability as well as performance. The key breakthrough came with latent diffusion: Vahdat et al. adopted their already NVAE work [^vahdat2020nvae] to propose LSGM [^vahdat_score-based_2021], a theoretically principled framework for joint VAE+diffusion training with tractable score matching and proper variational bounds. However, despite superior theory, LSGM's engineering complexity, including spectral regularization, careful hyperparameter tuning and variance reduction, limited practical adoption.

Rombach et al.'s Latent Diffusion Models (LDMs) [^rombach_high-resolution_2022] simplified this dramatically. Rather than joint end-to-end training, LDMs adopted a two-stage design: first, a VAE compresses images into lower-dimensional latents (typically $256 \times 256 \times 3 \to 32 \times 32 \times 4$); second, diffusion operates in this latent space. Critically, Rombach et al. used the empirically successful DDPM training recipe with simplified unweighted MSE loss rather than the full ELBO with likelihood weighting. This simplified approach, while breaking the connection to maximum likelihood training, produced better perceptual quality and more stable training, allowing extension of the approach to other modalities like video generation [^blattmann2023align][^brooks2024sora].

The modular two-stage approach provided significant advantages: VAEs pretrained once could be reused across different diffusion models, researchers could iterate independently on each component, and pretrained autoencoders from other work could be directly incorporated. This modularity accelerated research and deployment and enabled breakthroughs like Stable Diffusion XL [^podell2023sdxl]. However, as subsequent sections discuss, this separation between tokenizer and generative model is now being reconsidered.

Peebles and Xie's Diffusion Transformer (DiT) [^peebles2023scalable] demonstrated that transformers could replace U-Nets, achieving state-of-the-art ImageNet generation with favorable scaling. DiT operates on latent patches, treating them as sequences like Vision Transformers. A key finding: model complexity correlates strongly with sample quality—increasing depth, width, or tokens consistently improves generation. The largest DiT-XL/2 model established transformers as scalable alternatives for diffusion, serving as the baseline against which subsequent alignment methods would be measured.

Recent developments have also explored alternative generative paradigms. Flow matching [^lipman2024flow] provides a simulation-free approach to training continuous normalizing flows, offering faster training and sampling with better generalization than standard diffusion paths. These rectified flow transformers have been successfully scaled to production systems [^esser2024sd3]. The relationship between diffusion and flow matching has been clarified [^gao2024diffusion], showing they are fundamentally equivalent under certain conditions, differing primarily in parameterization and sampling schedules.

### Representation Learning: self-supervised vision foundation models

Self-supervised learning aims to learn general-purpose visual representations without manual labels, enabling models to exploit vast unlabeled corpora [^oord_representation_2019][^chen_exploring_2020][^caron_deep_2019][^caron_unsupervised_2021][^assran_self-supervised_2023][^huh_platonic_2024]. Early approaches were largely contrastive: they defined positive and negative pairs and trained encoders so that positives map to nearby features while negatives are pushed apart [^oord_representation_2019][^chen_exploring_2020][^chen2020simclr]. Subsequent work progressively weakened the dependence on labels, explicit negatives, and even pixel-level reconstruction, moving toward architectures that predict high-level, semantic representations [^caron_emerging_2021][^oquab_dinov2_2024][^simeoni_dinov3_2025][^assran_self-supervised_2023][^assran_v-jepa_2025].

#### From Cross-Modal Contrastive Learning to Single-Modal Contrastive Learning

A natural starting point for self-supervised representation learning is cross-modal contrastive learning, where aligned pairs provide supervision "for free." CLIP jointly trains image and text encoders so that the similarity between matching image–caption pairs is maximized and that between mismatched pairs is minimized, using a large-scale contrastive objective over Internet-scale image-text datasets [^radford2021learning][^brooks2024sora][^esser2024sd3]. This removes the need for class labels but depends on enormous amounts of paired data, and on sufficiently many negatives in each batch to avoid trivial solutions where the model encodes only coarse semantics [^zhai_sigmoid_2023].

SimCLR showed that contrastive learning can work in a purely single-modal setting [^chen2020simclr]. Two heavily augmented views of the same image form a positive pair, and all other images in the batch serve as negatives. Combined with strong data augmentation, a temperature-scaled InfoNCE loss, and large encoder capacity, SimCLR achieves supervised-level performance on ImageNet, demonstrating that labels are not strictly necessary for high-quality features. However, this comes at the cost of extremely large batch sizes, which are needed to provide enough negative examples so that the contrastive loss encourages fine-grained, non-trivial representations rather than collapsing to coarse global features [^oord_representation_2019].

SwAV improves on this regime by replacing explicit pairwise comparisons with online clustering [^caron_deep_2019][^caron_unsupervised_2021]. Instead of contrasting features directly, SwAV assigns representations to prototype clusters and enforces consistency of these assignments across multiple augmentations of the same image. This “swapped prediction” mechanism preserves many advantages of contrastive learning while being more memory-efficient and less sensitive to batch size, making it easier to scale to large datasets and long training schedules.

#### Vision–Language Models and Sigmoid Contrastive Losses

Vision-language pretraining extends contrastive learning to cross-modal settings at scale. CLIP demonstrated that large-scale image-text contrastive learning yields highly transferable visual representations and strong zero-shot performance across tasks [^radford2021learning][^brooks2024sora]. However, CLIP's softmax-based loss ties batch size directly to the number of effective negatives, which complicates scaling and makes training expensive.

SigLIP addresses this by replacing the softmax contrastive loss with a pairwise sigmoid loss over image-text similarities [^zhai_sigmoid_2023]. This loss operates independently on each pair, enabling smaller batch sizes while still learning strong fine-grained alignments between images and text. SigLIP 2 further augments this recipe by combining contrastive training with captioning-style objectives, self-supervised losses, and improved data mixtures, leading to better semantic understanding, localization, and dense prediction performance [^tschannen_siglip_2025].

#### Self-Distillation and Momentum Encoders

A key limitation of contrastive methods is their reliance on negatives. BYOL and related methods showed that it is possible to dispense with explicit negatives by using a momentum-updated teacher network [^grill_bootstrap_2020][^richemond_byol_2020]. The student is trained to match the teacher's representation of a differently augmented view of the same image; the teacher parameters are an exponential moving average of the student's, which stabilizes training and prevents collapse in practice.

DINO extends this self-distillation paradigm and reveals several surprising properties of the resulting representations [^caron_emerging_2021]. Without labels or negatives, DINO learns features whose attention maps correspond to object boundaries and support unsupervised semantic segmentation, indicating non-trivial semantic organization. In principle, such momentum-encoder methods require only a single image per batch, since supervision comes from matching teacher and student outputs rather than contrasting with other samples.

DINOv2 scales this recipe with larger Vision Transformers, improved optimization, and a carefully curated, diverse training set [^oquab_dinov2_2024]. The resulting models produce highly robust and transferable features that rival or surpass supervised pretraining across many benchmarks, as well as serving as strong vision foundation encoders for downstream tasks, including generative modeling [^skorokhodov_improving_2025][^chen_masked_2025]. However, prolonged self-distillation can gradually erode fine-grained spatial information, especially in dense feature maps used for pixel-level tasks.

To address this, DINOv3 introduces Gram anchoring, a regularization that stabilizes dense feature representations over long training schedules by constraining second-order statistics across patches and scales [^simeoni_dinov3_2025]. This mitigates the tendency of self-distillation to over-smooth features, preserving detailed structure that is crucial for dense prediction and generative tokenization while maintaining the semantic strengths of the DINO family.

#### Masked Image Modeling and Predictive Architectures

In parallel, masked image modeling treats images analogously to masked language modeling in NLP. MAE masks a large fraction of image patches (typically around 75%) and trains an asymmetric encoder-decoder architecture to reconstruct the missing pixels [^he2022masked]. This forces the encoder to focus on global structure rather than local texture, producing efficient representations that work well for many downstream tasks with modest finetuning.

iBOT combines masked prediction with self-distillation, using a teacher network as an online tokenizer that predicts semantic tokens for masked patches instead of raw pixels [^zhou2021ibot]. This hybrid objective closes much of the gap between contrastive and masked modeling approaches, yielding representations that perform strongly on both image-level classification and dense prediction.

Joint-embedding predictive architectures such as I-JEPA take a more explicitly semantic view: instead of reconstructing pixels, they predict high-level latent representations of masked regions from visible context [^assran_self-supervised_2023]. By operating entirely in representation space, I-JEPA avoids over-emphasizing low-level details and focuses learning on abstract structure, leading to scalable training and strong transfer across tasks.

V-JEPA 2 extends JEPA ideas to videos, pretraining on large-scale internet video and then lightly adapting with small amounts of robot data [^assran_v-jepa_2025]. The resulting models excel at motion understanding, action anticipation, and planning, illustrating that predictive architectures can serve as a unifying framework for perception and control when trained at scale on rich, largely unlabeled data [^assran_v-jepa_2025][^batatia2025foundation].

### Toward a Platonic Representation and Implications for Generative Models

Recent work from Philip Isola's lab at MIT has provided empirical evidence for a remarkable phenomenon: representations learned by different models, architectures, and even modalities converge toward a shared structure as models scale and training data diversifies [^huh_platonic_2024]. This convergent behavior has motivated the Platonic Representation Hypothesis, which posits that as models grow in capacity and are trained on increasingly rich data, their internal representations converge toward a shared statistical model of reality; a "platonic" representation that is largely independent of any specific task or architecture [^huh_platonic_2024].

The evidence for this convergence comes from multiple angles. The foundational work demonstrates that features from independently trained vision and language models become more aligned as scale and data diversity increase, and that different self-supervised objectives yield embeddings that occupy similar subspaces up to simple linear transformations [^huh_platonic_2024]. Subsequent research has shown that cross-modal training can benefit each modality individually: Gupta et al. demonstrate that leveraging unpaired multimodal data (e.g., text, audio, or images) consistently improves downstream performance in unimodal tasks, exploiting the assumption that different modalities are projections of a shared underlying reality [^gupta_better_2025]. Perhaps most strikingly, Wang et al. show that when language models are prompted with sensory instructions (e.g., "see" or "hear"), their representations become more similar to specialist vision and audio encoders, revealing that text-only models implicitly encode multimodal structure that can be activated through appropriate prompting [^wang_words_2025]. This suggests that even purely text-trained language models converge toward similar representations as vision models, with the convergence becoming stronger as models scale [^huh_platonic_2024][^wang_words_2025].

This hypothesis has direct implications for generative modeling. If discriminative vision foundation models such as DINOv2, DINOv3, and MAE converge toward an approximately optimal visual representation, then explicitly leveraging these encoders can accelerate the training and improve the quality of generative models that would otherwise have to discover similar structures from scratch [^oquab_dinov2_2024][^skorokhodov_improving_2025][^chen_masked_2025][^bi_vision_2025]. Recent work on aligning diffusion models to pretrained visual encoders—through feature alignment, representation regularization, or joint training of tokenizers and generators—can thus be viewed as an attempt to steer generative models toward this platonic representation early in training [^yu_representation_2025][^wu_representation_2025][^leng_repa-e_2025][^yao_reconstruction_2025][^wang_repa_2025][^wang_diffuse_2025]. This perspective sets the stage for our discussion of Phase 1 methods that explicitly align diffusion features to vision foundation models, and for later sections analyzing the emerging convergence between generative and discriminative representations.

## Phase 1: Aligning Diffusion Features to Vision Foundation Models

The first wave at the end of 2024/start of 2025 recognized that diffusion models learn semantically meaningful representations during training, but more slowly and less effectively than specialized discriminative models [^xiang_denoising_2023][^chen2024deconstructing]. The solution: align intermediate diffusion features with pretrained vision encoders to guide training.

REPA [^yu_representation_2025] introduced this paradigm in October 2024 through straightforward regularization. The method extracts features from intermediate diffusion layers, projects them through small MLPs, and maximizes cosine similarity with frozen DINOv2 encoder features. This auxiliary loss complements standard denoising:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{diffusion}} + \lambda \mathcal{L}_{\text{align}}
$$

The paper builds upon the insights from earlier work that diffusion models learn discriminative representations during denoising, but it takes the critical step to show that aligning these emerging representations with high-quality pretrained features accelerates convergence. Longer training improves weak natural alignment, but the REPA loss strengthens this alignment from the start, leading to better representations *and* better generation—a dual benefit suggesting a genuinely helpful inductive bias.

![REPA: Feature alignment during diffusion denoising](/assets/img/blog/r4g/LDM_to_REPA.png)
*Fig 4. Left shows baseline LDM architecture. Right shows REPA with alignment loss from intermediate diffusion features to frozen DINOv2 representations, speeding early training through semantic guidance.*

REG [^wu_representation_2025] extended this by entangling semantic class tokens with latent content during denoising. Rather than just aligning intermediate features, REG concatenates the [CLS] token from frozen DINOv2 with noisy latents, training the diffusion model to jointly reconstruct noise and original [CLS] token. This minimal overhead (single token, $<0.5\%$ FLOPs increase) provides stronger guidance than feature alignment alone. Interestingly, class token concatenation helps substantially even without explicit REPA alignment, though combining both works best—suggesting multiple mechanisms for incorporating semantic structure can be complementary.

![REG: Entangling class tokens with latents](/assets/img/blog/r4g/REPA_to_REG.png)
*Fig 5. REG concatenates the [CLS] token from frozen DINOv2 with noisy latents, enabling joint reconstruction of both image content and semantic class information directly from pure noise.*

HASTE [^wang_repa_2025] addressed a key REPA limitation: alignment helps dramatically early but can plateau or degrade later. Once the generative model begins modeling the full data distribution, the lower-dimensional discriminative teacher becomes a constraint rather than guide. The discriminative encoder focuses on task-relevant semantics while discarding generative details; forcing continued alignment may prevent learning the distribution's full complexity. HASTE introduces two-phase training: Phase I simultaneously distills attention maps (relational priors) and feature projections like REPA (semantic anchors) for rapid initial convergence. Phase II terminates alignment at a predetermined iteration, freeing the model to exploit its generative capacity. This simple modification achieves dramatic acceleration, with the key insight that alignment is most valuable for initial structure but counterproductive once basic semantic organization is learned.

![HASTE: Early-stopped alignment with staged termination](/assets/img/blog/r4g/REPA_to_HASTE.png)
*Fig 6. Phase I applies holistic alignment distilling both attention maps and features. Phase II terminates alignment one-shot at a fixed iteration, freeing the diffusion model to model the full distribution without the discriminative teacher constraint.*

Several puzzling trends emerged in representation alignment that defied conventional understanding. Larger model variants within the same encoder family often led to similar or even worse generation performance despite higher ImageNet-1K accuracy—DINOv2's larger variants showed diminishing returns, while PE and C-RADIO exhibited this counterintuitive pattern even more starkly[^yu_representation_2025]. More strikingly, representations with dramatically higher global semantic understanding consistently underperformed: PE-Core-G (82.8% ImageNet accuracy) generated worse images than PE-Spatial-B (53.1% accuracy), and SAM2-S achieved strong generation performance despite only 24.1% ImageNet accuracy - approximately 60% lower than many competing encoders. Perhaps most revealing, controlled experiments showed that explicitly injecting global information through CLS token mixing improved linear probing accuracy from 70.7% to 78.5% while simultaneously degrading generation quality, with FID worsening from 19.2 to 25.4.

iREPA's analysis in December 2025[^singh_what_2025] resolved these contradictions by demonstrating that spatial structure—the self-similarity patterns between patch tokens—not global semantics, drives representation alignment effectiveness. To quantify this insight, the authors measured spatial self-similarity structure [^shechtman2007matching] across patch tokens and performed large-scale correlation analysis across 27 vision encoders and three model sizes. Spatial structure metrics exhibited remarkably strong correlation with generation FID (Pearson |r| > 0.852 for metrics like Local Distance Similarity, Short-Range Spatial Similarity, Cosine Distance Similarity, and Relative Mean Spatial Contrast), far exceeding ImageNet-1K accuracy's predictive power (|r| = 0.26).
​
This explained SAM2's paradoxical success: despite poor classification accuracy, it maintained strong spatial structure that proved ideal for generation. The authors then took this further and made two small modifications to the REPA recipe: by replacing standard MLP projection with convolutional layers that preserve local spatial relationships and implementing spatial normalization to accentuate relational structure transfer, iREPA (implemented in fewer than 4 lines of code) consistently improves convergence speed across diverse encoders, model sizes, and training recipes including REPA, REPA-E (more on this later), and MeanFlow (a few-step training method). This take-away This aligns with HASTE's emphasis on attention distillation: the success lies in teaching spatial organization coherence rather than transferring high-level semantic concepts.


Phase 1 establishes clear patterns: Using pretrained vision foundation models through representation alignment dramatically accelerates diffusion training. However, the methods operate at the level of intermediate diffusion features, leaving the VAE latent space unchanged. Phase 2 takes the logical next step: incorporating semantic structure into the latent space itself.

## Phase 2: Aligning the VAE Latent Space to Foundation Models

While Phase 1 aligned intermediate diffusion features, Phase 2 recognized that the latent space itself—the compressed VAE representation—could incorporate semantic structure from vision foundation models. This deeper integration addresses the fundamental trade-off between reconstruction quality and learnability of the latent distribution.

![The optimization dilemma in latent diffusion](/assets/img/blog/r4g/good_latent_space.png)
*Fig 7. LSGM (2021) aims for smooth trajectories in latent space by normalizing distributions. LDM (2022) emphasizes highly compressed latents for computational efficiency. EQ-VAE and VA-VAE tackle the trade-off: improving encoder equivariance and aligning latent encodings with pretrained models to create learnable high-dimensional spaces. Image kindly adapted from Arash Vahdat.*

Standard LDM treats VAE and diffusion training as independent, with the VAE optimized solely for pixel reconstruction (and perceptual quality by auxiliary losses relying on discriminators or metrics like LPIPS [^dieleman2025latents]). This pixel-focused objective produces latents encoding low-level details effectively but lacking semantic structure. Increasing latent dimensionality improves reconstruction but creates higher-dimensional, more complex spaces for diffusion to learn—an "optimization dilemma" where better reconstruction leads to harder generation.

![VA-VAE: Aligning VAE latents to VFM during tokenizer training](/assets/img/blog/r4g/LDM_to_VAVAE.png)
*Fig 8. VAE encoder trains with both reconstruction loss and alignment loss to frozen VFM features, creating latents that are both reconstructive and semantically meaningful for efficient diffusion model training.*

VA-VAE [^yao_reconstruction_2025] directly tackles this: it aligns the VAE's latent space with pretrained vision foundation models during tokenizer training rather than relying solely on pixels via their VF loss:

$$
\mathcal{L}_{\text{VA-VAE}} = \mathcal{L}_{\text{recon}} + \beta \cdot \text{KL} + \lambda_{\text{align}} \mathcal{L}_{\text{VF}}.
$$

$$
\mathcal{L}_{\text{VF}} = w_{\text{hyper}} \, w_{\text{adaptive}} \Big( \mathcal{L}_{\text{mcos}} + \mathcal{L}_{\text{mdms}} \Big).
$$

with

$$
\mathcal{L}_{\text{mcos}} = \frac{1}{h w} \sum_{i=1}^{h} \sum_{j=1}^{w} \operatorname{ReLU} \!\left( 1 - m_{1} - \frac{z'_{ij} \cdot f_{ij}} {\lVert z'_{ij}\rVert \,\lVert f_{ij}\rVert} \right),
$$

$$
\mathcal{L}_{\text{mdms}} = \frac{1}{N^{2}} \sum_{i,j} \operatorname{ReLU} \left( \left| \frac{z_i \cdot z_j}{\lVert z_i\rVert \,\lVert z_j\rVert} - \frac{f_i \cdot f_j}{\lVert f_i\rVert \,\lVert f_j\rVert} \right| - m_{2} \right),
$$

where $Z$ is the VAE latent feature map and $F$ is the frozen vision foundation feature map for the same image; $Z' = WZ$ linearly projects VAE latents to the VFM feature dimension; $z'_{ij}$ and $f_{ij}$ are the projected latent and VFM feature at spatial position $(i,j)$; $z_i, f_i$ are vectors at position $i$ after flattening the $h \times w$ grid into $N = h w$ tokens; $m_1$ and $m_2$ are cosine-similarity margins; $\mathcal{L}_{\text{mcos}}$ enforces pointwise alignment, $\mathcal{L}_{\text{mdms}}$ aligns pairwise relational structure; $w_{\text{adaptive}} = \lVert\nabla \mathcal{L}_{\text{recon}}\rVert / \lVert\nabla \mathcal{L}_{\text{VF,raw}}\rVert$ rescales VF gradients to match the reconstruction loss, and $w_{\text{hyper}}$ is a user-set scalar (e.g., $0.1$) controlling the overall VF strength.

The VF loss encourages both point-by-point alignment (individual latent vectors close to VFM features) and relative alignment (relationships between latents match relationships between features), using adaptive weighting similar in spirit to loss balancing in GANs [^goodfellow2014generative]. This yields semantically organized high-dimensional latent spaces that retain reconstruction quality while being more learnable for downstream generative models.

By semantically structuring the latent space of the VAE, it reduces the diffusion model's burden, allowing it to focus on learning the distribution rather than also discovering semantic organization. The latent space provides appropriate inductive bias—semantic structure "baked in" through VFM alignment while pixel details are captured through reconstruction.

![REPA-E: End-to-end joint VAE+diffusion training](/assets/img/blog/r4g/REPA_to_REPAE.png)
*Fig 10. Left shows stage-wise training with frozen VAE. Right shows REPA-E with careful gradient flow: alignment loss flows to both components, diffusion loss uses stop-gradient on VAE encoder, and VAE receives alignment gradients through BatchNorm for latent normalization.*

After diffusion model alignment as well as VAE alignment had been demonstrated, REPA-E [^leng_repa-e_2025] takes integration further through joint VAE+diffusion training, challenging the convention that these components should train separately. It demonstrates that while naive end-to-end training with diffusion loss alone is ineffective (causing latent space collapse), representation alignment provides necessary constraints for successful joint optimization. The key innovation proved to be careful gradient control. Alignment loss flows to both VAE and diffusion model, but diffusion loss uses stop-gradient on the VAE encoder to prevent collapse (the VAE shouldn't change to make diffusion easier at reconstruction's cost). In addition, to keep the latent space normalised, the VAE receives alignment gradients only through BatchNorm normalisation. This enables joint optimization: the VAE improves to produce latents both reconstructive *and* well-aligned, while the diffusion model learns in this evolving but stable space. Joint optimization improves the VAE itself, leading to better latent structure (higher VFM alignment, better class separation) and downstream performance. While in LSGM [^vahdat_score-based_2021] pretraining of the VAE was necessary, true end-to-end training is now possible.


3stage-aligner [^chen_aligning_2025] proposes an alternative strategy: rather than training the VAE from scratch with alignment, freeze a pretrained encoder (e.g., DINOv2), map into into a low-dimension space via an adapter block and learn to align a decoder through three stages. Stage 1 (Latent Alignment) freezes the VFM encoder and trains the adapter plus decoder, establishing a semantic latent space with basic reconstruction capabilities. The resulting latents are semantically grounded but exhibit color shifts and missing fine-grained details since the frozen encoder was not trained for reconstruction. Stage 2 (Perceptual Alignment) jointly optimizes adapter and encoder (now unfrozen) with semantic preservation loss maintaining alignment with original frozen VFM features:

$$
\mathcal{L}_{\text{Stage 2}} = \mathcal{L}_{\text{recon}} + \lambda_2 \lVert\text{Enc}(x) - \text{Enc}_{\text{frozen}}(x) \rVert^2
$$

The L2 loss prevents encoder drift from the pretrained semantic structure while allowing capture of fine-grained color and texture. Stage 3 (Decoder Refinement) freezes both encoder and adapter, allowing the decoder to better exploit the latent representation changed during Stage 2 without disturbing semantic structure.

![3stage-aligner: Three-stage frozen encoder alignment](/assets/img/blog/r4g/VFMVAE_to_3stagealigner.png)
*Fig 11. Stage 1 establishes semantic grounding with frozen encoder. Stage 2 allows encoder refinement with semantic preservation loss. Stage 3 optimizes decoder for reconstruction quality, carefully balancing semantic preservation with fine-grained detail capture.*

This yields semantically rich tokenizers where latent space inherits discriminative structure from the pretrained encoder. The three-stage process carefully balances semantic preservation (maintaining VFM structure) with reconstruction quality (capturing fine-grained details), avoiding color shifts of purely frozen encoders and semantic drift of fully unconstrained fine-tuning.

Phase 2 establishes that incorporating semantic structure directly into VAE latent space—whether through alignment loss during training (VA-VAE), end-to-end joint optimization (REPA-E), or staged adaptation of frozen encoders (3stage-aligner)—produces superior results compared to standard pixel-focused VAE training. These semantically-structured spaces are easier to learn (faster convergence) and produce better final quality. However, they still rely on the two-stage VAE+diffusion pipeline, raising a natural question: do we need VAE compression at all?

## Phase 3: Operating Directly in Vision Foundation Model Feature Spaces

The third phase represents a more radical departure, questioning whether the VAE bottleneck is necessary at all. Instead of compressing images through a VAE and then aligning the latent space, these methods propose directly using pretrained vision foundation model features as the "latent space" for diffusion, or training autoencoders specifically to preserve discriminative information rather than minimize reconstruction error.

![Phase 3 overview: Eliminating VAE compression](/assets/img/blog/r4g/diffuse_in_embedding_space.png)
*Fig 12. VFM-VAE uses frozen VFM features with multi-scale fusion and progressive reconstruction. SVG uses frozen DINO with residual encoder for fine-grained details. RAE replaces VAE entirely with pretrained representation encoders.*

Based on the observation made in Perception Encoder [^bolya_perception_2025] that the best visual embeddings for downstream tasks are often not at the output of vision networks but rather in intermediate layers, VFM-VAE [^bi_vision_2025] merges frozen VFM features from different parts of the network as latent representations. However, VFMs focus on semantic understanding, producing spatially coarse features (e.g., DINOv2 ViT-L outputs $16 \times 16$ for $256 \times 256$ images) sacrificing pixel fidelity. VFM-VAE redesigns the decoder with multi-scale latent fusion (combining features from multiple VFM layers, providing both semantic guidance from deep layers and spatial detail from shallow layers) and progressive resolution reconstruction (building up resolution gradually through decoder blocks, starting from coarse VFM features and progressively adding detail). In addition, the embedding dimensionality of VFMs is often too high for effective generative modelling; VFM-VAE circumvents this by mapping the different embeddings into a compressed latent space that is regularised via KL divergence, thereby still containing a VAE but with strong initialisation by a VFM.

![VFM-VAE: Leveraging multiple VFM encodings as compressed latents](/assets/img/blog/r4g/VAVAE_to_VFMVAE.png)
*Fig 9. In VFM-VAE, multiple VFM encodings are compressed into a single latent representation that is then projected out to pixel space via multi-scale decoders. The right side shows that this latent space is more robust to geometric perturbations and achieves strong reconstruction as well as generation.*

This enables high-quality reconstruction from semantically rich but spatially compact representations. The work also introduces SE-CKNNA metric for diagnosing representation dynamics during diffusion training. SE-CKNNA measures how well semantic structure in latent space is preserved during noising, revealing that semantic structure degrades nonlinearly with noise level, with critical thresholds where class separability breaks down. Using these insights, the authors develop joint tokenizer-diffusion alignment strategy dramatically accelerating convergence. The frozen pretrained encoder ensures the latent space maintains semantic alignment even under distribution shifts—Phase 2 methods that fine-tune encoders risk semantic drift; VFM-VAE's frozen encoder ensures consistent structure. However, this requires architectural innovations (multi-scale fusion, progressive reconstruction) to overcome reconstruction challenges of coarse frozen features, which prevents easy adoption.

SVG [^shi_latent_2025] tries to avoid these complex architectural modifications by taking a principled approach analyzing why VAE latent spaces are problematic: they lack clear semantic separation and strong discriminative structure. Standard VAE latents exhibit semantic entanglement (different classes overlap) and poor class compactness (same-class samples widely dispersed). This makes the distribution difficult for diffusion to learn, as it must simultaneously discover semantic structure and model fine-grained variation. To overcome this, SVG constructs latent representations from frozen DINO features providing semantically discriminative structure with clear class separation, augmented with lightweight residual branch capturing fine-grained details:

$$
z_{\text{final}} = z_{\text{DINO}} + \alpha \cdot z_{\text{residual}}
$$

where frozen DINOv2 provides semantics and a learned residual encoder captures color, texture, and other details DINO discards. Normal VAE latents are semantically entangled, but alignment to VFM models enables clearer class separation and more compact classes. The SVG encoder proves important for fine-grained color details. No diffusion model tricks are needed since in the case of the chosen VFM DINOv3, the latent space is small enough (384-dimensional) to be modelled without compression. However, the alignment loss is crucial: without it, the decoder over-relies on the residual encoder, and numerical range differences between normalized frozen DINOv3 features and unnormalized learned residuals can distort semantic embeddings.

![SVG: Using frozen DINO with residual encoder](/assets/img/blog/r4g/VAVAE_to_SVG.png)
*Fig 13. Left shows VA-VAE with learned encoder aligned to VFM. Right shows SVG with frozen DINO encoder plus lightweight residual encoder capturing fine-grained details, enabling clearer semantic separation without VAE training.*

While SVG emphasises the need for a modest embedding space dimensionality and the need for a residual encoder that makes up for missing pixel-level details in the VFM embeddings, RAE [^zheng_diffusion_2025] tries to replace the VAE solely with pretrained representation encoders paired with trained decoders, without additional compression or auxiliary encoders. The authors systematically explore encoders from diverse self-supervised methods (DINO, SigLIP, MAE) and analyze challenges of operating diffusion transformers in resulting high-dimensional spaces. While standard VAE latents are low-dimensional ($32 \times 32 \times 4$, or 4K dimensions), representation encoder outputs are much higher ($16 \times 16 \times 1024$ for DINOv2 ViT-L, or 262K dimensions). This poses challenges for diffusion transformers that generally perform poorly in such high-dimensional spaces.

RAE identifies and addresses sources of difficulty through theoretically motivated solutions. First, standard DiT bottlenecks all tokens through the same hidden dimension, so when input tokens have higher dimensionality, this creates an information bottleneck. RAE introduces a wide DDT head that maintains high-dimensional representations through a final shallow-but-wide layer while keeping the majority of the DiT block lower-dimensional. Second, standard schedules are designed based on spatial dimensions assuming certain statistical properties. Representation encoder outputs have different characteristics (already normalized, different variance structure). Therefore, RAE makes the noise schedule depend on actual data statistics rather than assuming fixed properties. Third, since the decoder trains separately from the frozen encoder, mismatch can occur at inference—the diffusion model produces slightly imperfect samples, but the decoder was trained on clean representations. Following TarFlow [^zhai2024normalizing], RAE adds noise augmentation during decoder training for robustness to imperfect samples.

RAE demonstrated that high-quality reconstruction from frozen DINO encoders with strong representations is possible. Computational overhead is minimal since DiT cost depends mostly on sequence length, not token dimension (which the wide head addresses). The DiT adjustments are necessary: scaling width to token dimension, making noise schedule data-dependent instead of spatial-dependent, and using noise-augmented decoding due to discrete decoder training. An additional benefit of RAE is that high-resolution synthesis is trivially enabled by swapping decoders with different patch sizes—the frozen encoder and trained diffusion model remain unchanged.

![RAE: Comprehensive framework for representation autoencoders](/assets/img/blog/r4g/SVG_to_RAE.png)
*Fig 14. Shows systematic exploration of different pretrained encoders (DINO, SigLIP, MAE) as frozen latent encoders, with DiT adjustments (wide DDT head, data-dependent noise schedule, noise-augmented decoding) enabling effective diffusion in high-dimensional representation spaces.*

However, while RAE allowed the direct use of pretrained VFMs as encoders, it has two main limitations:
1. The modifications of the diffusion model required to make this work were substantial.
2. There is no emphasis whatsoever on reconstruction, limiting editing capabilities of these models and making them potentially vulnerable to drifting off the data manifold.

FAE [^gao_one_2025] focuses on tackling the first of these challenges by introducing a simple adoption via a single attention layer that allows the usage of standard LightningDiT recipes. By then training to both reconstruct images and preserve pretrained features, FAE creates truly unified representation serving as both generative latent space and discriminative feature space. The simple translation layer (a single attention layer between frozen encoder features and generative decoder) provides minimal but effective transformation. This allows use of standard diffusion models again without the RAE modifications, demonstrating that the right architectural intervention can eliminate the need for extensive model adjustments. It also shows that the simple translation layer preserves the spatial structure in latent space, which aligns with the iREPA insights that spatial structure is the main determinant for how effective alignment will be for generation quality [^singh_what_2025].

![FAE: Streamlined unification of feature and generative spaces](/assets/img/blog/r4g/RAE_to_FAE.png)
*Fig 15. Unlike RAE, FAE introduces a lightweight "translation layer" (a single attention block) to align frozen pretrained encoder features with the generative decoder. This minimal intervention preserves spatial structure and discriminative power.*

While FAE tackled the architectural adoption problem of RAE, PS-VAE [^zhang2025psvae] tackled the editing problem that comes with the fact that RAE does not encourage the latent space to encode reconstruction capability explicitly. By training sequential representation as well as pixel decoders as well as finetuning the pretrained representation encoder with reconstruction losses, they find a good balance between reconstruction and representation capabilities and show that this balance allows them to perform superior generation and editing.

Most recently, UAE [^fan2025harmonizing] offers a theoretical unification through its "Prism Hypothesis," which posits that semantic and pixel representations correspond to different frequency bands of a shared spectrum. Unlike SVG which adds a separate residual encoder, or RAE which relies on a heavy decoder, UAE initializes its encoder from DINOv2 and utilizes a frequency-band modulator to disentangle the latent space. It explicitly aligns the low-frequency band to the semantic teacher while dedicating high-frequency bands to residual details, effectively harmonizing semantic abstraction with pixel fidelity in a single compact latent space.

Phase 3 methods establish that VAE compression is not fundamental to high-quality latent diffusion. By directly using pretrained vision foundation model features as latent representations (with appropriate architectural modifications handling high-dimensionality, spatial coarseness, and reconstruction challenges), we achieve generation quality comparable to or exceeding VAE-based methods while maintaining discriminative power of the original pretrained encoder. However, all Phase 3 methods still rely on pretrained vision foundation models. Phase 4 takes the final step: questioning whether we need pretrained representations at all.

## Phase 4: Questioning the Need for Pretrained Representations

After three phases focused on progressively sophisticated ways to leverage pretrained models, Phase 4 represents a countertrend: can we achieve similar benefits by training from scratch with better objectives and architectures? This phase questions whether dependency on external pretrained models is fundamental or merely a workaround for suboptimal training procedures.

USP [^chu_usp_2025] embodies this philosophy through fully end-to-end training jointly optimized for both generative and discriminative objectives. Rather than initializing from external representations, it employs a multi-task loss combining generation and discrimination such as contrastive learning, masked prediction, or classification. Generative and discriminative objectives complement one another: generative learning encourages modeling the full data distribution, while discriminative tasks promote the discovery of semantically meaningful structure. Joint optimization thus produces representations that are simultaneously generative (capable of synthesis) and discriminative (useful downstream), reducing the reliance on separate pretraining stages. This raises a critical question: does representation alignment solve deep architectural deficiencies, or does it merely accelerate learning? If the latter, the necessity of pretrained models could wane as compute, data, and training recipes continue to scale.

A similar spirit underlies large-scale systems such as FLUX2-VAE [^noauthor_black_nodate], which demonstrates that sophisticated tokenizers can be learned directly through end-to-end training rather than depending on pretrained vision foundation features. Although little is publicly known about its technical details, FLUX2-VAE's production success suggests that with sufficient scale and engineering, high-quality tokenizers and representations can emerge organically from task training alone. Yet, "without pretrained representations" does not necessarily mean "cheap to train": the total computational cost may rival or even exceed that of conventional pretraining pipelines. Whether the elegance of end-to-end architectures outweighs the modularity, interpretability, and reusability of pretrained components remains an open question.

The same shift is visible in the recent renaissance of pixel-space diffusion models, which challenge the long-held assumption that latent diffusion is a prerequisite for high-resolution, high-quality generation. Methods such as JiT (Just image Transformer) [^li_back_2025], PixelDiT [^yu_pixeldit_2025], DeCo (frequency-DeCoupled diffusion) [^ma_deco_2025], DiP (Diffusion in Pixel space) [^chen_dip_2025], and SiD2 (Simpler Diffusion v2) [^hoogeboom_simpler_2025] illustrate a broader trend: architectural innovation can substitute for latent-space compression. By employing patch-based Transformers, efficient multi-scale attention, or frequency-aware loss designs, these models demonstrate that the efficiency, quality, and stability advantages traditionally attributed to latent spaces can also be achieved through direct pixel-space training. The result is a growing recognition that high-resolution generative performance depends less on where the model operates (latent or pixel space) and more on how it is structured and optimized.

EPG [^lei_advancing_2025] pushes this idea further by integrating representation learning into pixel-space diffusion itself. Rather than discarding the notion of learned structure, it reimagines representation pretraining as part of the diffusion process. EPG pretrains encoders through self-supervised objectives along deterministic diffusion trajectories, learning temporally consistent and semantically distinct features directly in pixel space. This pretraining endows the encoder with structured initialization analogous to pretrained vision models, but derived natively from the diffusion task. The result is a model that successfully trains consistency and diffusion systems from scratch, reportedly the first to achieve stable training of high-resolution consistency models without any pretrained VAEs or diffusion models.

Together, these innovations in pixel-space diffusion reinforce Phase 4's premise: high-quality representations and generative performance need not rely on external pretraining. As compute and data availability scale, architectural ingenuity and task-specific pretraining increasingly bridge the gap once filled by large pretrained backbones. However, fully end-to-end models remain computationally demanding and fragile during optimization. In contrast, pretrained components provide stability and reduce redundant computation across research efforts. Empirically, pretrained models still dominate in efficiency at moderate scales, while end-to-end approaches begin to overtake them only at extreme scales. In practice, the future may lean toward hybrid systems, i.e pretrained models for rapid development and exploration, and large-scale, task-specific end-to-end training for production models where the additional compute cost yields tangible performance gains.

## Generative Models as Representations

An alternative perspective on unifying generation and representation learning is to use generative modeling itself as the pretraining objective, treating the features learned during generative training as useful representations for downstream tasks.

MAE [^he2022masked] pioneered masked pixel reconstruction for vision, demonstrating that predicting masked image patches creates strong representations. However, the pixel-level reconstruction objective tends to focus on low-level details rather than high-level semantics. Could predicting embeddings instead of pixels yield better representations?

AIM v1 (Autoregressive Image Models) [^el2024scalable] revisits autoregressive modeling for vision with modern architectures and large-scale data. Unlike early work like iGPT [^chen2020generative] or D-iGPT [^ren2023rejuvenating], AIM uses Vision Transformers and is trained on billions of images. The work demonstrates two key findings: (1) visual feature performance scales with both model capacity and data quantity, exhibiting similar scaling laws to large language models, and (2) the value of the autoregressive objective function correlates with downstream performance, providing a meaningful training signal. AIM-7B achieves 84.0% ImageNet fine-tuning accuracy and shows particularly strong performance when trained on diverse, uncurated web data. The autoregressive objective—predicting the next patch given previous patches—naturally creates representations that capture sequential dependencies and can be more easily scaled than traditional contrastive or masked modeling approaches.

AIM v2 [^fini2025multimodal] extends this to multimodal autoregressive models, demonstrating that the same autoregressive paradigm can be applied across images and text, creating unified representations that span modalities.

NEPA (Next-Embedding Prediction) [^xu_next-embedding_2025] borrows the AIM idea of predicting embeddings from pretrained models rather than raw pixels or image patches. By operating in a semantic embedding space, NEPA focuses on high-level features rather than low-level details, potentially leading to more efficient and semantically meaningful representation learning. The authors suggest that this approach could be extended to operate in latent spaces, bridging generative objectives with the representation-focused methods discussed in earlier sections.

The broader pattern here is that generative objectives—whether autoregressive, masked, or diffusion-based—can serve dual purposes: they enable sampling of new examples and, as a byproduct, learn representations useful for discriminative tasks. Recent work on improving diffusion autoencoders [^skorokhodov_improving_2025] and using masked autoencoders as tokenizers [^chen_masked_2025] further blurs the line between generative and representation learning, suggesting these are complementary views of the same underlying learning process.

Additional relevant work includes methods that explicitly bridge generative and discriminative training. Robust representation consistency models [^lei_robust_2025] use contrastive denoising to learn consistent representations along diffusion trajectories, improving both robustness and downstream performance. The dispersive loss [^wang_diffuse_2025] provides a simple plug-and-play regularizer that encourages diffusion model representations to disperse in hidden space (analogous to contrastive learning) without requiring positive pairs, improving generation quality without interfering with the sampling process and was used in previously mentioned pixel-space diffusion models like EPG [^lei_advancing_2025].

## Representation Learning and Alignment in Molecular Machine Learning

The ideas from visual representation learning and generative modeling are beginning to influence molecular and protein modeling, suggesting broader applicability of these concepts beyond computer vision. In particular, there is a growing interest in molecular representation spaces that play a role analogous to DINO or SigLIP in vision, serving as generic, pretrained feature spaces for a wide range of downstream tasks [^bernstein2024gap][^li2025platonic].

MACE (Message Passing Atomic Cluster Expansion) has emerged as a foundation model for atomistic materials chemistry, combining equivariant message passing with systematically improvable many-body basis functions [^batatia2025foundation][^bernstein2024gap]. Trained on large collections of quantum-mechanical reference data, MACE learns rich local representations of atomic environments that generalize across diverse chemistries, phases, and thermodynamic conditions, thereby playing an analogous role to vision foundation models in the molecular domain [^batatia2025foundation]. Crucially, these representations seem to generalise well across molecular modalities, allowing not only accurate property predictions in materials (the original field of study) but also molecules [^wedig2025rem3di] and proteins [^bojan2025representing].

MACE-REPA directly applies the representation alignment paradigm to molecular force fields, mirroring Phase 1 methods developed for diffusion models in vision [^pinede2025unifying]. Instead of aligning diffusion features to DINO-like vision encoders, MACE-REPA aligns the intermediate representations learned during molecular dynamics force-field training with pretrained MACE features, using auxiliary losses that encourage consistency between the evolving force-field encoder and a frozen foundation potential. This demonstrates that the core insight of representation alignment—leveraging structured pretrained representations to accelerate and stabilize training—transfers robustly from image diffusion models to atomistic simulations.

Beyond a single model family, unified molecular feature spaces have been explored by multiple groups, echoing the Platonic Representation Hypothesis in vision. Work from MIT demonstrates that ostensibly different molecular models, trained on overlapping quantum-chemistry corpora, can be mapped into a common latent space with minimal loss in predictive performance, indicating that they encode closely related underlying chemical manifolds [^edamadaka2025universally]. Complementary work from London analyzes the geometry of feature spaces learned by NNPs such as MACE and related architectures, providing empirical evidence that independently trained models converge toward similar "platonic" representations of molecular structure and energetics [^li2025platonic]. Together, these results suggest that there are strong constraints on how molecular structure and forces can be represented, and that sufficiently expressive models trained on large, diverse datasets tend to discover comparable latent organizations, a similar trend as in computer vision.

In parallel, people also try to construct custom VAEs for representation learning in the biomolecular domain, similar to the FLUX-line of work in computer vision. One such example is SLAE [^chen2025slae]: rather than encoding entire molecules or proteins as global graphs, SLAE constructs latent representations at the level of strictly local all-atom environments (with a locality bias similar to MACE that could allow stronger generalisation), and trains them with a combination of reconstruction losses and auxiliary property objectives such as hydration energies, force-field terms, or stability proxies. This places SLAE conceptually close to recent image-space autoencoders that blend reconstruction and semantic objectives: reconstruction ensures geometric fidelity, while the auxiliary heads encourage the latent space to align with physically meaningful axes, thereby bridging representation and reconstruction in an analogous way to aligned VAEs and representation autoencoders in vision.

## Conclusion

The field has undergone rapid evolution, progressing through four distinct phases: (1) aligning diffusion features with pretrained representations, (2) incorporating semantic structure into VAE latent spaces, (3) directly using pretrained representations as latent spaces, and (4) questioning whether pretrained representations are necessary at all. Parallel developments in pixel-space diffusion and generative representation learning have further enriched the landscape.

Several clear patterns emerge: representation alignment dramatically accelerates training, spatial structure may be more important than global semantics, VAE compression is not fundamental, and principles transfer beyond vision to molecular modeling. However, fundamental questions remain: What makes representations learnable? What is the optimal compression rate? How do we unify multiple modalities? Should we train jointly or in stages?

The answers likely depend on scale, application requirements, and computational constraints. At research scale, leveraging pretrained models provides clear advantages. At production scale, end-to-end training may be preferable despite higher cost. For maximum quality, pixel-space methods avoid reconstruction artifacts. For maximum efficiency, latent-space methods reduce computation.

Looking forward, I am very excited to see how these advances in vision will translate to the molecular world. I myself have worked quite a bit on generative modelling for proteins [^geffner2025proteina] and small molecules [^schneuing2024structure], recently also leveraging latent diffusion [^geffner2025laproteina], so I will follow this space with great interest!

## References

[^ho2020ddpm]: Ho, J., et al. (2020). Denoising Diffusion Probabilistic Models. *NeurIPS*.
[^song2020score]: Song, Y., et al. (2020). Score-Based Generative Modeling through Stochastic Differential Equations. *ICLR*.
[^lai2025principles]: Lai, C.-H., Song, Y., Kim, D., Mitsufuji, Y., & Ermon, S. (2025). The principles of diffusion models. *arXiv preprint arXiv:2510.21890*. [https://arxiv.org/abs/2510.21890](https://arxiv.org/abs/2510.21890)
[^lipman2024flow]: Lipman, Y., et al. (2024). Flow Matching for Generative Modeling. *ICLR*.
[^albergo2023building]: Albergo, M. S., & Vanden-Eijnden, E. (2023). Building Normalizing Flows with Stochastic Interpolants. *ICLR*.
[^radford2021learning]: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. *ICML*.
[^caron_emerging_2021]: Caron, M., et al. (2021). Emerging Properties in Self-Supervised Vision Transformers. *ICCV*.
[^he2022masked]: He, K., et al. (2022). Masked Autoencoders Are Scalable Vision Learners. *CVPR*.
[^rombach_high-resolution_2022]: Rombach, R., et al. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR*.
[^blattmann2023align]: Blattmann, A., et al. (2023). Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. *CVPR*.
[^brooks2024sora]: Brooks, T., et al. (2024). Video generation models as world simulators. *OpenAI*.
[^podell2023sdxl]: Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. *ICLR*.
[^peebles2023scalable]: Peebles, W., & Xie, S. (2023). Scalable Diffusion Models with Transformers. *ICCV*.
[^esser2024sd3]: Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. *ICML*.
[^gao2024diffusion]: Gao, R., Hoogeboom, E., Heek, J., Bortoli, V. D., Murphy, K. P., & Salimans, T. (2024). Diffusion meets flow matching: Two sides of the same coin. *arXiv preprint arXiv:2401.08740*. [https://arxiv.org/abs/2401.08740](https://arxiv.org/abs/2401.08740)
[^oord_representation_2019]: Oord, A. v. d., Li, Y., & Vinyals, O. (2019). Representation Learning with Contrastive Predictive Coding. *arXiv preprint arXiv:1807.03748*. [https://arxiv.org/abs/1807.03748](https://arxiv.org/abs/1807.03748)
[^chen_exploring_2020]: Chen, X., & He, K. (2020). Exploring Simple Siamese Representation Learning. *CVPR*.
[^caron_deep_2019]: Caron, M., et al. (2019). Deep Clustering for Unsupervised Learning of Visual Features. *ECCV*.
[^caron_unsupervised_2021]: Caron, M., et al. (2021). Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. *NeurIPS*.
[^assran_self-supervised_2023]: Assran, M., et al. (2023). Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. *CVPR*.
[^huh_platonic_2024]: Huh, M., Cheung, B., Wang, T., & Isola, P. (2024). The Platonic Representation Hypothesis. *arXiv preprint arXiv:2405.07987*. [https://arxiv.org/abs/2405.07987](https://arxiv.org/abs/2405.07987)
[^chen2020simclr]: Chen, T., et al. (2020). A Simple Framework for Contrastive Learning of Visual Representations. *ICML*.
[^assran_v-jepa_2025]: Assran, M., et al. (2025). V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning. *arXiv preprint arXiv:2506.09985*. [https://arxiv.org/abs/2506.09985](https://arxiv.org/abs/2506.09985)
[^oquab_dinov2_2024]: Oquab, M., et al. (2024). DINOv2: Learning Robust Visual Features without Supervision. *arXiv preprint arXiv:2304.07193*. [https://arxiv.org/abs/2304.07193](https://arxiv.org/abs/2304.07193)
[^simeoni_dinov3_2025]: Simeoni, O., et al. (2025). DINOv3. *arXiv preprint arXiv:2508.10104*. [https://arxiv.org/abs/2508.10104](https://arxiv.org/abs/2508.10104)
[^zhai_sigmoid_2023]: Zhai, X., et al. (2023). Sigmoid Loss for Language Image Pre-Training. *ICCV*.
[^tschannen_siglip_2025]: Tschannen, M., et al. (2025). SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features. *arXiv preprint arXiv:2502.14786*. [https://arxiv.org/abs/2502.14786](https://arxiv.org/abs/2502.14786)
[^bolya_perception_2025]: Bolya, D., et al. (2025). Perception Encoder: The best visual embeddings are not at the output of the network. *arXiv preprint arXiv:2504.13181*. [https://arxiv.org/abs/2504.13181](https://arxiv.org/abs/2504.13181)
[^grill_bootstrap_2020]: Grill, J.-B., et al. (2020). Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. *NeurIPS*.
[^richemond_byol_2020]: Richemond, P. H., et al. (2020). BYOL works even without batch statistics. *arXiv preprint arXiv:2010.10241*. [https://arxiv.org/abs/2010.10241](https://arxiv.org/abs/2010.10241)
[^skorokhodov_improving_2025]: Skorokhodov, I., et al. (2025). Improving the Diffusability of Autoencoders. *arXiv preprint arXiv:2502.14831*. [https://arxiv.org/abs/2502.14831](https://arxiv.org/abs/2502.14831)
[^chen_masked_2025]: Chen, H., et al. (2025). Masked Autoencoders Are Effective Tokenizers for Diffusion Models. *arXiv preprint arXiv:2502.03444*. [https://arxiv.org/abs/2502.03444](https://arxiv.org/abs/2502.03444)
[^bi_vision_2025]: Bi, T., Zhang, X., Lu, Y., & Zheng, N. (2025). Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models. *arXiv preprint arXiv:2510.18457*. [https://arxiv.org/abs/2510.18457](https://arxiv.org/abs/2510.18457)
[^zhou2021ibot]: Zhou, J., et al. (2021). iBOT: Image BERT Pre-Training with Online Tokenizer. *ICLR*.
[^batatia2025foundation]: Batatia, I., et al. (2025). A foundation model for atomistic materials chemistry. *The Journal of Chemical Physics*, 163(18). [https://arxiv.org/abs/2406.11627](https://arxiv.org/abs/2406.11627)
[^yu_representation_2025]: Yu, S., et al. (2025). Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think. *arXiv preprint arXiv:2410.06940*. [https://arxiv.org/abs/2410.06940](https://arxiv.org/abs/2410.06940)
[^wu_representation_2025]: Wu, G., et al. (2025). Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think. *arXiv preprint arXiv:2507.01467*. [https://arxiv.org/abs/2507.01467](https://arxiv.org/abs/2507.01467)
[^leng_repa-e_2025]: Leng, X., Singh, J., Hou, Y., Xing, Z., Xie, S., & Zheng, L. (2025). REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers. *arXiv preprint arXiv:2504.10483*. [https://arxiv.org/abs/2504.10483](https://arxiv.org/abs/2504.10483)
[^yao_reconstruction_2025]: Yao, J., Yang, B., & Wang, X. (2025). Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models. *arXiv preprint arXiv:2501.01423*. [https://arxiv.org/abs/2501.01423](https://arxiv.org/abs/2501.01423)
[^wang_repa_2025]: Wang, Z., et al. (2025). REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training. *arXiv preprint arXiv:2505.16792*. [https://arxiv.org/abs/2505.16792](https://arxiv.org/abs/2505.16792)
[^wang_diffuse_2025]: Wang, R., & He, K. (2025). Diffuse and Disperse: Image Generation with Representation Regularization. *arXiv preprint arXiv:2506.09027*. [https://arxiv.org/abs/2506.09027](https://arxiv.org/abs/2506.09027)
[^xiang_denoising_2023]: Xiang, W., et al. (2023). Denoising Diffusion Autoencoders are Unified Self-Supervised Learners. *ICCV*.
[^chen2024deconstructing]: Chen, X., Liu, Z., Xie, S., & He, K. (2024). Deconstructing denoising diffusion models for self-supervised learning. *arXiv preprint arXiv:2401.14404*. [https://arxiv.org/abs/2401.14404](https://arxiv.org/abs/2401.14404)
[^singh_what_2025]: Singh, J., Leng, X., Wu, Z., Zheng, L., Zhang, R., Shechtman, E., & Xie, S. (2025). What matters for Representation Alignment: Global Information or Spatial Structure? *arXiv preprint arXiv:2512.10794*. [https://arxiv.org/abs/2512.10794](https://arxiv.org/abs/2512.10794)
[^shechtman2007matching]: Shechtman, E., & Irani, M. (2007). Matching local self-similarities across images and videos. *2007 IEEE Conference on Computer Vision and Pattern Recognition* (pp. 1--8). IEEE.
[^dieleman2025latents]: Dieleman, S. (2025). Generative modelling in latent space. [https://sander.ai/2025/04/15/latents.html](https://sander.ai/2025/04/15/latents.html)
[^goodfellow2014generative]: Goodfellow, I., et al. (2014). Generative Adversarial Networks. *NeurIPS*.
[^chen_aligning_2025]: Chen, B., et al. (2025). Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models. *arXiv preprint arXiv:2509.25162*. [https://arxiv.org/abs/2509.25162](https://arxiv.org/abs/2509.25162)
[^shi_latent_2025]: Shi, M., et al. (2025). Latent Diffusion Model without Variational Autoencoder. *arXiv preprint arXiv:2510.15301*. [https://arxiv.org/abs/2510.15301](https://arxiv.org/abs/2510.15301)
[^zheng_diffusion_2025]: Zheng, B., Ma, N., Tong, S., & Xie, S. (2025). Diffusion Transformers with Representation Autoencoders. *arXiv preprint arXiv:2510.11690*. [https://arxiv.org/abs/2510.11690](https://arxiv.org/abs/2510.11690)
[^zhai2024normalizing]: Zhai, S., Zhang, R., Nakkiran, P., Berthelot, D., Gu, J., Zheng, H., Chen, T., Bautista, M. A., Jaitly, N., & Susskind, J. (2024). Normalizing flows are capable generative models. *arXiv preprint arXiv:2412.06329*. [https://arxiv.org/abs/2412.06329](https://arxiv.org/abs/2412.06329)
[^gao_one_2025]: Gao, Y., Chen, C., Chen, T., & Gu, J. (2025). One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation. *arXiv preprint arXiv:2512.07829*. [https://arxiv.org/abs/2512.07829](https://arxiv.org/abs/2512.07829)
[^zhang2025psvae]: Zhang, S. (2025). Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing. *arXiv preprint arXiv:2509.25162*. [https://arxiv.org/abs/2509.25162](https://arxiv.org/abs/2509.25162)
[^fan2025harmonizing]: Fan, W., Diao, H., Wang, Q., Lin, D., & Liu, Z. (2025). The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding. *arXiv preprint arXiv:2512.19693*. [https://arxiv.org/abs/2512.19693](https://arxiv.org/abs/2512.19693)
[^chu_usp_2025]: Chu, X., Li, R., & Wang, Y. (2025). USP: Unified Self-Supervised Pretraining for Image Generation and Understanding. *arXiv preprint arXiv:2503.06132*. [https://arxiv.org/abs/2503.06132](https://arxiv.org/abs/2503.06132)
[^noauthor_black_nodate]: Black Forest Labs. (n.d.). FLUX. [https://bfl.ai/research/representation-comparison](https://bfl.ai/research/representation-comparison)
[^li_back_2025]: Li, T., & He, K. (2025). Back to Basics: Let Denoising Generative Models Denoise. *arXiv preprint arXiv:2511.13720*. [https://arxiv.org/abs/2511.13720](https://arxiv.org/abs/2511.13720)
[^yu_pixeldit_2025]: Yu, Y., Xiong, W., Nie, W., Sheng, Y., Liu, S., & Luo, J. (2025). PixelDiT: Pixel Diffusion Transformers for Image Generation. *arXiv preprint arXiv:2511.20645*. [https://arxiv.org/abs/2511.20645](https://arxiv.org/abs/2511.20645)
[^ma_deco_2025]: Ma, Z., Wei, L., Wang, S., Zhang, S., & Tian, Q. (2025). DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation. *arXiv preprint arXiv:2511.19365*. [https://arxiv.org/abs/2511.19365](https://arxiv.org/abs/2511.19365)
[^chen_dip_2025]: Chen, Z., et al. (2025). DiP: Taming Diffusion Models in Pixel Space. *arXiv preprint arXiv:2511.18822*. [https://arxiv.org/abs/2511.18822](https://arxiv.org/abs/2511.18822)
[^hoogeboomsimpler2025]: Hoogeboom, E., Mensink, T., Heek, J., Lamerigts, K., Gao, R., & Salimans, T. (2025). Simpler Diffusion (SiD2): 1.5 FID on ImageNet512 with pixel-space diffusion. *arXiv preprint arXiv:2410.19324*. [https://arxiv.org/abs/2410.19324](https://arxiv.org/abs/2410.19324)
[^lei_advancing_2025]: Lei, J., Liu, K., Berner, J., Yu, H., Zheng, H., Wu, J., & Chu, X. (2025). Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training. *arXiv preprint arXiv:2510.12586*. [https://arxiv.org/abs/2510.12586](https://arxiv.org/abs/2510.12586)
[^el2024scalable]: El-Nouby, A., Klein, M., Zhai, S., Bautista, M. A., Toshev, A., Shankar, V., Susskind, J. M., & Joulin, A. (2024). Scalable pre-training of large autoregressive image models. *arXiv preprint arXiv:2401.08541*. [https://arxiv.org/abs/2401.08541](https://arxiv.org/abs/2401.08541)
[^chen2020generative]: Chen, M., et al. (2020). Generative Pretraining from Pixels. *ICML*.
[^ren2023rejuvenating]: Ren, S., Wang, Z., Zhu, H., Xiao, J., Yuille, A., & Xie, C. (2023). Rejuvenating image-gpt as strong visual representation learners. *arXiv preprint arXiv:2312.02147*. [https://arxiv.org/abs/2312.02147](https://arxiv.org/abs/2312.02147)
[^fini2025multimodal]: Fini, E., Shukor, M., Li, X., Dufter, P., Klein, M., Haldimann, D., Aitharaju, S., da Costa, V. G. T., Béthune, L., Gan, Z., et al. (2025). Multimodal autoregressive pre-training of large vision encoders. *Proceedings of the Computer Vision and Pattern Recognition Conference*, 9641-9654.
[^xu_next-embedding_2025]: Xu, S., Ma, Z., Chai, W., Chen, X., Jin, W., Chai, J., Xie, S., & Yu, S. X. (2025). Next-Embedding Prediction Makes Strong Vision Learners. *arXiv preprint arXiv:2512.16922*. [https://arxiv.org/abs/2512.16922](https://arxiv.org/abs/2512.16922)
[^lei_robust_2025]: Lei, J., Berner, J., Wang, J., Chen, Z., Ba, Z., Ren, K., Zhu, J., & Anandkumar, A. (2025). Robust Representation Consistency Model via Contrastive Denoising. *arXiv preprint arXiv:2501.13094*. [https://arxiv.org/abs/2501.13094](https://arxiv.org/abs/2501.13094)
[^bernstein2024gap]: Bernstein, N. (2024). From GAP to ACE to MACE. *arXiv preprint arXiv:2410.06354*. [https://arxiv.org/abs/2410.06354](https://arxiv.org/abs/2410.06354)
[^li2025platonic]: Li, Z., & Walsh, A. (2025). Platonic representation of foundation machine learning interatomic potentials. *arXiv preprint arXiv:2512.05349*. [https://arxiv.org/abs/2512.05349](https://arxiv.org/abs/2512.05349)
[^wedig2025rem3di]: Wedig, S., Elijošius, R., Schran, C., & Schaaf, L. L. (2025). REM3DI: Learning smooth, chiral 3D molecular representations from equivariant atomistic foundation models. *NeurIPS 2025 Workshop on Symmetry and Geometry in Neural Representations*.
[^bojan2025representing]: Bojan, M., Vedula, S., Maddipatla, A., Sellam, N. B., Napoli, F., Schanda, P., & Bronstein, A. M. (2025). Representing local protein environments with atomistic foundation models. *arXiv preprint arXiv:2505.23354*. [https://arxiv.org/abs/2505.23354](https://arxiv.org/abs/2505.23354)
[^pinede2025unifying]: Pinede, L., Yang, S., Nam, J., & Gomez-Bombarelli, R. (2025). Unifying Force Prediction and Molecular Conformation Generation Through Representation Alignment. *ICML 2025 Generative AI and Biology (GenBio) Workshop*.
[^edamadaka2025universally]: Edamadaka, S., Yang, S., Li, J., & Gómez-Bombarelli, R. (2025). Universally Converging Representations of Matter Across Scientific Foundation Models. *arXiv preprint arXiv:2512.03750*. [https://arxiv.org/abs/2512.03750](https://arxiv.org/abs/2512.03750)
[^chen2025slae]: Chen, Y., et al. (2025). SLAE: Strictly Local All-atom Environment. *bioRxiv*.
[^kadkhodaie_unconditional_2025]: Kadkhodaie, Z., Mallat, S., & Simoncelli, E. (2025). Unconditional CNN denoisers contain sparse semantic representation of images. *arXiv preprint arXiv:2506.01912*. [https://arxiv.org/abs/2506.01912](https://arxiv.org/abs/2506.01912)
[^liang_how_2024]: Liang, Q., Liu, Z., Ostrow, M., & Fiete, I. (2024). How Diffusion Models Learn to Factorize and Compose. *arXiv preprint arXiv:2408.13256*. [https://arxiv.org/abs/2408.13256](https://arxiv.org/abs/2408.13256)
[^jaini_intriguing_2024]: Jaini, P., Clark, K., & Geirhos, R. (2024). Intriguing properties of generative classifiers. *arXiv preprint arXiv:2309.16779*. [https://arxiv.org/abs/2309.16779](https://arxiv.org/abs/2309.16779)
[^hoogeboomsimple2023]: Hoogeboom, E., Heek, J., & Salimans, T. (2023). Simple diffusion: End-to-end diffusion for high resolution images. *arXiv preprint arXiv:2301.11093*. [https://arxiv.org/abs/2301.11093](https://arxiv.org/abs/2301.11093)
[^gupta_better_2025]: Gupta, S., Sundaram, S., Wang, C., Jegelka, S., & Isola, P. (2025). Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models. *arXiv preprint arXiv:2510.08492*. [https://arxiv.org/abs/2510.08492](https://arxiv.org/abs/2510.08492)
[^wang_words_2025]: Wang, S. L., Isola, P., & Cheung, B. (2025). Words That Make Language Models Perceive. *arXiv preprint arXiv:2510.02425*. [https://arxiv.org/abs/2510.02425](https://arxiv.org/abs/2510.02425)
[^geffner2025proteina]: Geffner, T., Didi, K., Zhang, Z., Reidenbach, D., Cao, Z., Yim, J., Geiger, M., Dallago, C., Kucukbenli, E., Vahdat, A., & others. (2025). Proteina: Scaling flow-based protein structure generative models. *arXiv preprint arXiv:2503.00710*. [https://arxiv.org/abs/2503.00710](https://arxiv.org/abs/2503.00710)
[^geffner2025laproteina]: Geffner, T., Didi, K., Cao, Z., Reidenbach, D., Zhang, Z., Dallago, C., Kucukbenli, E., Kreis, K., & Vahdat, A. (2025). La-proteina: Atomistic protein generation via partially latent flow matching. *arXiv preprint arXiv:2507.09466*. [https://arxiv.org/abs/2507.09466](https://arxiv.org/abs/2507.09466)
[^schneuing2024structure]: Schneuing, A., Harris, C., Du, Y., Didi, K., Jamasb, A., Igashov, I., Du, W., Gomes, C., Blundell, T. L., Lio, P., & others. (2024). Structure-based drug design with equivariant diffusion models. *Nature Computational Science*, 4(12), 899--909. [https://www.nature.com/articles/s43588-024-00680-9](https://www.nature.com/articles/s43588-024-00680-9)


