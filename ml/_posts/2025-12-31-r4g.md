---
layout: post
title: The unification of representation learning and generative modelling
image: /assets/img/blog/r4g/overview_figure.png
accent_image: 
  background: url('/assets/img/blog/jj-ying.png') center/cover
  overlay: false
accent_color: '#ccc'
theme_color: '#ccc'
description: >
  A deep dive into the convergence of discriminative and generative AI, covering 4 phases of evolution from REPA to Pixel-Space Diffusion.
invert_sidebar: true
categories: ml
---

# The unification of representation learning and generative modelling

## Introduction

Both generative modeling and representation learning have made impressive advances in recent years, particularly in computer vision. Diffusion [^ho2020ddpm][^song2020score][^lai2025principles] and flow models [^lipman2024flow][^albergo2023building] have achieved unprecedented generation quality, while self-supervised paradigms like CLIP [^radford2021learning], DINO [^caron_emerging_2021], and MAE [^he2022masked] have enabled state-of-the-art performance on classification, detection, and depth estimation. Yet generation has remained separate from other vision tasks, raising a natural question: can we create unified representations useful for both discriminative and generative tasks?

![Evolution of representation learning and generative modeling (2019-2025)](/assets/img/blog/r4g/timeline.png)
*Fig 1. Three parallel timelines showing the independent evolution of representation learning methods, latent generative modeling architectures, and the recent convergence of these fields in R4G (Representation for Generation).*

This field—sometimes termed Representation for Generation (R4G)—has evolved rapidly over the past year, with multiple groups independently converging on similar insights. The rapid development reveals fundamental questions about visual representations: Are features learned during generation inherently different from those learned discriminatively? Can we bridge these paradigms for more efficient systems? Recent evidence suggests diffusion models already learn semantically meaningful representations [^kadkhodaie_unconditional_2025][^liang_how_2024], and that generative classifiers exhibit surprisingly human-like properties [^jaini_intriguing_2024].

![Four phases of representation-guided generation](/assets/img/blog/r4g/overview_figure.png)
*Fig 2. Evolution from no alignment (Phase 0) through feature alignment (Phase 1), VAE alignment (Phase 2), VAE-less direct embedding diffusion (Phase 3), and pixel-space diffusion without pretrained models (Phase 4). Each block shows the architectural evolution and which papers introduced key innovations.*

We organize recent developments into four phases reflecting the evolution of thinking: from initial alignment strategies to questioning whether pretrained representations are necessary at all. We also explore important tangential developments in pixel-space versus latent diffusion models and the emerging use of generative models for representation learning. Finally, we discuss how these ideas are beginning to influence molecular machine learning, suggesting broader applicability beyond computer vision.

* toc
{:toc}

## Background

### Latent Diffusion Models

![LDM training pipeline](/assets/img/blog/r4g/ldm_diagram.png)
*Fig 3. Two-stage training: first, a VAE compresses images into latent space; second, diffusion operates in this latent space. This modular design accelerated adoption but separated tokenizer training from diffusion model training.*

Diffusion models revolutionized generation by framing it as iterative denoising. Ho et al. [^ho2020ddpm] introduced DDPMs that gradually add noise and learn to reverse it; Song et al. [^song2020score] formalized this through stochastic differential equations. The key breakthrough came with latent diffusion: Vahdat et al.'s LSGM [^vahdat_score-based_2021] provided a theoretically principled framework for joint VAE+diffusion training with tractable score matching and proper variational bounds. However, despite superior theory, LSGM's engineering complexity—spectral regularization, careful hyperparameter tuning, variance reduction—limited practical adoption.

Rombach et al.'s Latent Diffusion Models (LDMs) [^rombach_high-resolution_2022] simplified this dramatically. Rather than joint end-to-end training, LDMs adopted a two-stage design: first, a VAE compresses images into lower-dimensional latents (typically $256 \times 256 \times 3 \to 32 \times 32 \times 4$); second, diffusion operates in this latent space. Critically, Rombach et al. used the empirically successful DDPM training recipe with simplified unweighted MSE loss rather than the full ELBO with likelihood weighting. This simplified approach, while breaking the connection to maximum likelihood training, produced better perceptual quality and more stable training, allowing extension of the approach to other modalities like video generation [^blattmann2023align][^brooks2024sora].

The modular two-stage approach provided significant advantages: VAEs pretrained once could be reused across different diffusion models, researchers could iterate independently on each component, and pretrained autoencoders from other work could be directly incorporated. This modularity accelerated research and deployment and enabled breakthroughs like Stable Diffusion XL [^podell2023sdxl]. However, as subsequent sections discuss, this separation between tokenizer and generative model is now being reconsidered.

Peebles and Xie's Diffusion Transformer (DiT) [^peebles2023scalable] demonstrated that transformers could replace U-Nets, achieving state-of-the-art ImageNet generation with favorable scaling. DiT operates on latent patches, treating them as sequences like Vision Transformers. A key finding: model complexity correlates strongly with sample quality—increasing depth, width, or tokens consistently improves generation. The largest DiT-XL/2 model established transformers as scalable alternatives for diffusion, serving as the baseline against which subsequent alignment methods would be measured.

Recent developments have also explored alternative generative paradigms. Flow matching [^lipman2024flow] provides a simulation-free approach to training continuous normalizing flows, offering faster training and sampling with better generalization than standard diffusion paths. These rectified flow transformers have been successfully scaled to production systems [^esser2024sd3]. The relationship between diffusion and flow matching has been clarified [^gao2024diffusion], showing they are fundamentally equivalent under certain conditions, differing primarily in parameterization and sampling schedules.

### Representation Learning

Self-supervised learning aims to learn general-purpose visual representations without manual labels, enabling models to exploit vast unlabeled corpora [^oord_representation_2019][^chen_exploring_2020][^caron_deep_2019][^caron_unsupervised_2021][^assran_self-supervised_2023][^huh_platonic_2024]. Early approaches were largely contrastive: they defined positive and negative pairs and trained encoders so that positives map to nearby features while negatives are pushed apart [^oord_representation_2019][^chen_exploring_2020][^chen2020simclr]. Subsequent work progressively weakened the dependence on labels, explicit negatives, and even pixel-level reconstruction, moving toward architectures that predict high-level, semantic representations [^caron_emerging_2021][^oquab_dinov2_2024][^simeoni_dinov3_2025][^assran_self-supervised_2023][^assran_v-jepa_2025].

#### From Cross-Modal Contrastive Learning to Single-Modal Contrastive Learning

A natural starting point for self-supervised representation learning is cross-modal contrastive learning, where aligned pairs provide supervision "for free." CLIP jointly trains image and text encoders so that the similarity between matching image–caption pairs is maximized and that between mismatched pairs is minimized, using a large-scale contrastive objective over Internet-scale image-text datasets [^radford2021learning][^brooks2024sora][^esser2024sd3]. This removes the need for class labels but depends on enormous amounts of paired data, and on sufficiently many negatives in each batch to avoid trivial solutions where the model encodes only coarse semantics [^radford2021learning][^zhai_sigmoid_2023].

SimCLR showed that contrastive learning can work in a purely single-modal setting [^chen2020simclr]. Two heavily augmented views of the same image form a positive pair, and all other images in the batch serve as negatives. Combined with strong data augmentation, a temperature-scaled InfoNCE loss, and large encoder capacity, SimCLR achieves supervised-level performance on ImageNet, demonstrating that labels are not strictly necessary for high-quality features [^chen2020simclr][^chen_exploring_2020]. However, this comes at the cost of extremely large batch sizes, which are needed to provide enough negative examples so that the contrastive loss encourages fine-grained, non-trivial representations rather than collapsing to coarse global features [^chen2020simclr][^oord_representation_2019].

SwAV improves on this regime by replacing explicit pairwise comparisons with online clustering [^caron_deep_2019][^caron_unsupervised_2021]. Instead of contrasting features directly, SwAV assigns representations to prototype clusters and enforces consistency of these assignments across multiple augmentations of the same image. This “swapped prediction” mechanism preserves many advantages of contrastive learning while being more memory-efficient and less sensitive to batch size, making it easier to scale to large datasets and long training schedules.

#### Vision–Language Models and Sigmoid Contrastive Losses

Vision-language pretraining extends contrastive learning to cross-modal settings at scale. CLIP demonstrated that large-scale image-text contrastive learning yields highly transferable visual representations and strong zero-shot performance across tasks [^radford2021learning][^brooks2024sora]. However, CLIP's softmax-based loss ties batch size directly to the number of effective negatives, which complicates scaling and makes training expensive [^radford2021learning].

SigLIP addresses this by replacing the softmax contrastive loss with a pairwise sigmoid loss over image-text similarities [^zhai_sigmoid_2023][^tschannen_siglip_2025]. This loss operates independently on each pair, enabling smaller batch sizes while still learning strong fine-grained alignments between images and text [^zhai_sigmoid_2023]. SigLIP 2 further augments this recipe by combining contrastive training with captioning-style objectives, self-supervised losses, and improved data mixtures, leading to better semantic understanding, localization, and dense prediction performance [^tschannen_siglip_2025][^bolya_perception_2025].

#### Self-Distillation and Momentum Encoders

A key limitation of contrastive methods is their reliance on negatives. BYOL and related methods showed that it is possible to dispense with explicit negatives by using a momentum-updated teacher network [^grill_bootstrap_2020][^richemond_byol_2020]. The student is trained to match the teacher's representation of a differently augmented view of the same image; the teacher parameters are an exponential moving average of the student's, which stabilizes training and prevents collapse in practice.

DINO extends this self-distillation paradigm and reveals several surprising properties of the resulting representations [^caron_emerging_2021]. Without labels or negatives, DINO learns features whose attention maps correspond to object boundaries and support unsupervised semantic segmentation, indicating non-trivial semantic organization [^caron_emerging_2021][^huh_platonic_2024]. In principle, such momentum-encoder methods require only a single image per batch, since supervision comes from matching teacher and student outputs rather than contrasting with other samples.

DINOv2 scales this recipe with larger Vision Transformers, improved optimization, and a carefully curated, diverse training set [^oquab_dinov2_2024][^bolya_perception_2025]. The resulting models produce highly robust and transferable features that rival or surpass supervised pretraining across many benchmarks, as well as serving as strong vision foundation encoders for downstream tasks, including generative modeling [^skorokhodov_improving_2025][^chen_masked_2025]. However, prolonged self-distillation can gradually erode fine-grained spatial information, especially in dense feature maps used for pixel-level tasks [^simeoni_dinov3_2025].

To address this, DINOv3 introduces Gram anchoring, a regularization that stabilizes dense feature representations over long training schedules by constraining second-order statistics across patches and scales [^simeoni_dinov3_2025]. This mitigates the tendency of self-distillation to over-smooth features, preserving detailed structure that is crucial for dense prediction and generative tokenization while maintaining the semantic strengths of the DINO family [^bi_vision_2025].

#### Masked Image Modeling and Predictive Architectures

In parallel, masked image modeling treats images analogously to masked language modeling in NLP. MAE masks a large fraction of image patches (typically around 75%) and trains an asymmetric encoder-decoder architecture to reconstruct the missing pixels [^he2022masked][^bolya_perception_2025]. This forces the encoder to focus on global structure rather than local texture, producing efficient representations that work well for many downstream tasks with modest finetuning [^he2022masked].

iBOT combines masked prediction with self-distillation, using a teacher network as an online tokenizer that predicts semantic tokens for masked patches instead of raw pixels [^zhou2021ibot][^caron_emerging_2021]. This hybrid objective closes much of the gap between contrastive and masked modeling approaches, yielding representations that perform strongly on both image-level classification and dense prediction [^zhou2021ibot][^caron_emerging_2021][^chen_masked_2025].

Joint-embedding predictive architectures such as I-JEPA take a more explicitly semantic view: instead of reconstructing pixels, they predict high-level latent representations of masked regions from visible context [^assran_self-supervised_2023][^huh_platonic_2024]. By operating entirely in representation space, I-JEPA avoids over-emphasizing low-level details and focuses learning on abstract structure, leading to scalable training and strong transfer across tasks [^assran_self-supervised_2023][^oquab_dinov2_2024].

V-JEPA 2 extends JEPA ideas to videos, pretraining on large-scale internet video and then lightly adapting with small amounts of robot data [^assran_v-jepa_2025]. The resulting models excel at motion understanding, action anticipation, and planning, illustrating that predictive architectures can serve as a unifying framework for perception and control when trained at scale on rich, largely unlabeled data [^assran_v-jepa_2025][^batatia2025foundation].

#### Toward a Platonic Representation and Implications for Generative Models

The convergent behavior of representations across architectures, objectives, and modalities has motivated more theoretical perspectives on self-supervised learning. The Platonic Representation Hypothesis posits that, as models grow in capacity and are trained on increasingly rich data, their internal representations converge toward a shared statistical model of reality; a "platonic" representation that is largely independent of any specific task or architecture [^huh_platonic_2024]. Empirical evidence supports this view: features from independently trained vision and language models become more aligned as scale and data diversity increase, and different self-supervised objectives yield embeddings that occupy similar subspaces up to simple linear transformations [^huh_platonic_2024][^oquab_dinov2_2024][^caron_emerging_2021].

This hypothesis has direct implications for generative modeling. If discriminative vision foundation models such as DINOv2, DINOv3, and MAE converge toward an approximately optimal visual representation, then explicitly leveraging these encoders can accelerate the training and improve the quality of generative models that would otherwise have to discover similar structures from scratch [^oquab_dinov2_2024][^skorokhodov_improving_2025][^chen_masked_2025][^bi_vision_2025]. Recent work on aligning diffusion models to pretrained visual encoders—through feature alignment, representation regularization, or joint training of tokenizers and generators—can thus be viewed as an attempt to steer generative models toward this platonic representation early in training [^yu_representation_2025][^wu_representation_2025][^leng_repa-e_2025][^yao_reconstruction_2025][^wang_repa_2025][^wang_diffuse_2025]. This perspective sets the stage for our discussion of Phase 1 methods that explicitly align diffusion features to vision foundation models, and for later sections analyzing the emerging convergence between generative and discriminative representations.

## Phase 1: Aligning Diffusion Features to Vision Foundation Models

The first wave recognized that diffusion models learn semantically meaningful representations during training, but more slowly and less effectively than specialized discriminative models [^xiang_denoising_2023][^chen2024deconstructing]. The solution: align intermediate diffusion features with pretrained vision encoders to guide training.

REPA [^yu_representation_2025] introduced this paradigm through straightforward regularization. The method extracts features from intermediate diffusion layers, projects them through small MLPs, and maximizes cosine similarity with frozen DINOv2 encoder features. This auxiliary loss complements standard denoising:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{diffusion}} + \lambda \mathcal{L}_{\text{align}}
$$

The key insight: diffusion models learn discriminative representations during denoising, and aligning these emerging representations with high-quality pretrained features accelerates convergence. Analysis reveals that diffusion models do learn representations comparable to DINOv2, but alignment dramatically speeds this process. Longer training improves weak natural alignment, but the REPA loss strengthens this alignment from the start, leading to better representations *and* better generation—a dual benefit suggesting a genuinely helpful inductive bias.

![REPA: Feature alignment during diffusion denoising](/assets/img/blog/r4g/LDM_to_REPA.png)
*Fig 4. Left shows baseline LDM architecture. Right shows REPA with alignment loss from intermediate diffusion features to frozen DINOv2 representations, speeding early training through semantic guidance.*

REG [^wu_representation_2025] extended this by entangling semantic class tokens with latent content during denoising. Rather than just aligning intermediate features, REG concatenates the [CLS] token from frozen DINOv2 with noisy latents, training the diffusion model to jointly reconstruct noise and original [CLS] token. This minimal overhead (single token, $<0.5\%$ FLOPs increase) provides stronger guidance than feature alignment alone. Interestingly, class token concatenation helps substantially even without explicit REPA alignment, though combining both works best—suggesting multiple mechanisms for incorporating semantic structure can be complementary.

![REG: Entangling class tokens with latents](/assets/img/blog/r4g/REPA_to_REG.png)
*Fig 5. REG concatenates the [CLS] token from frozen DINOv2 with noisy latents, enabling joint reconstruction of both image content and semantic class information directly from pure noise.*

HASTE [^wang_repa_2025] addressed a key REPA limitation: alignment helps dramatically early but can plateau or degrade later. Once the generative model begins modeling the full data distribution, the lower-dimensional discriminative teacher becomes a constraint rather than guide. The discriminative encoder focuses on task-relevant semantics while discarding generative details; forcing continued alignment may prevent learning the distribution's full complexity. HASTE introduces two-phase training: Phase I simultaneously distills attention maps (relational priors) and feature projections (semantic anchors) for rapid initial convergence. Phase II performs one-shot termination of alignment at a predetermined iteration, freeing the model to exploit its generative capacity. This simple modification achieves dramatic acceleration, with the key insight that alignment is most valuable for initial structure but counterproductive once basic semantic organization is learned.

![HASTE: Early-stopped alignment with staged termination](/assets/img/blog/r4g/REPA_to_HASTE.png)
*Fig 6. Phase I applies holistic alignment distilling both attention maps and features. Phase II terminates alignment one-shot at a fixed iteration, freeing the diffusion model to model the full distribution without the discriminative teacher constraint.*

Subsequent work questioned what aspect of target representations drives these gains. iREPA's analysis [^singh_what_2025] showed that *spatial structure*, not global semantics, is key. By replacing standard MLP projection with convolutional layers preserving local spatial relationships and implementing attention map distillation transferring relational structure, iREPA consistently improves upon REPA. This aligns with HASTE's emphasis on attention distillation: the success lies in teaching spatial organization coherence rather than transferring high-level semantic concepts. This insight clarified a few trends that were not explained well beforehand: certain features that improve discriminatory power of representations as measured by e.g. classification accuracy (injection of global semantics via CLS tokens, larger model sizes, global instead of local supervision losses) often reduced the generative power of models aligned to these embedding spaces, which iREPA convincingly demonstrated to be due to the destroyed spatial structure.

Phase 1 establishes clear patterns: Using pretrained vision foundation models through representation alignment dramatically accelerates diffusion training. However, the methods operate at the level of intermediate diffusion features, leaving the VAE latent space unchanged. Phase 2 takes the logical next step: incorporating semantic structure into the latent space itself.

## Phase 2: Aligning the VAE Latent Space to Foundation Models

While Phase 1 aligned intermediate diffusion features, Phase 2 recognized that the latent space itself—the compressed VAE representation—could incorporate semantic structure from vision foundation models. This deeper integration addresses the fundamental trade-off between reconstruction quality and learnability of the latent distribution.

![The optimization dilemma in latent diffusion](/assets/img/blog/r4g/good_latent_space.png)
*Fig 7. LSGM (2021) aims for smooth trajectories in latent space by normalizing distributions. LDM (2022) emphasizes highly compressed latents for computational efficiency. EQ-VAE and VA-VAE tackle the trade-off: improving encoder equivariance and aligning latent encodings with pretrained models to create learnable high-dimensional spaces.*

Standard LDM treats VAE and diffusion training as independent, with the VAE optimized solely for pixel reconstruction (and perceptual quality by auxiliary losses relying on discriminators or metrics like LPIPS [^dieleman2025latents]). This pixel-focused objective produces latents encoding low-level details effectively but lacking semantic structure. Increasing latent dimensionality improves reconstruction but creates higher-dimensional, more complex spaces for diffusion to learn—an "optimization dilemma" where better reconstruction leads to harder generation.

![VA-VAE: Aligning VAE latents to VFM during tokenizer training](/assets/img/blog/r4g/LDM_to_VAVAE.png)
*Fig 8. VAE encoder trains with both reconstruction loss and alignment loss to frozen VFM features, creating latents that are both reconstructive and semantically meaningful for efficient diffusion model training.*

VA-VAE [^yao_reconstruction_2025] directly tackles this: it aligns the VAE's latent space with pretrained vision foundation models during tokenizer training rather than relying solely on pixels:

$$
\mathcal{L}_{\text{VA-VAE}} = \mathcal{L}_{\text{recon}} + \beta \cdot \text{KL} + \lambda_{\text{align}} \mathcal{L}_{\text{VF}}.
$$

$$
\mathcal{L}_{\text{VF}} = w_{\text{hyper}} \, w_{\text{adaptive}} \Big( \mathcal{L}_{\text{mcos}} + \mathcal{L}_{\text{mdms}} \Big).
$$

with

$$
\mathcal{L}_{\text{mcos}} = \frac{1}{h w} \sum_{i=1}^{h} \sum_{j=1}^{w} \operatorname{ReLU} \!\left( 1 - m_{1} - \frac{z'_{ij} \cdot f_{ij}} {\lVert z'_{ij}\rVert \,\lVert f_{ij}\rVert} \right),
$$

$$
\mathcal{L}_{\text{mdms}} = \frac{1}{N^{2}} \sum_{i,j} \operatorname{ReLU} \left( \left| \frac{z_i \cdot z_j}{\lVert z_i\rVert \,\lVert z_j\rVert} - \frac{f_i \cdot f_j}{\lVert f_i\rVert \,\lVert f_j\rVert} \right| - m_{2} \right),
$$

where $Z$ is the VAE latent feature map and $F$ is the frozen vision foundation feature map for the same image; $Z' = WZ$ linearly projects VAE latents to the VFM feature dimension; $z'_{ij}$ and $f_{ij}$ are the projected latent and VFM feature at spatial position $(i,j)$; $z_i, f_i$ are vectors at position $i$ after flattening the $h \times w$ grid into $N = h w$ tokens; $m_1$ and $m_2$ are cosine-similarity margins; $\mathcal{L}_{\text{mcos}}$ enforces pointwise alignment, $\mathcal{L}_{\text{mdms}}$ aligns pairwise relational structure; $w_{\text{adaptive}} = \lVert\nabla \mathcal{L}_{\text{recon}}\rVert / \lVert\nabla \mathcal{L}_{\text{VF,raw}}\rVert$ rescales VF gradients to match the reconstruction loss, and $w_{\text{hyper}}$ is a user-set scalar (e.g., $0.1$) controlling the overall VF strength.

The VF loss encourages both point-by-point alignment (individual latent vectors close to VFM features) and relative alignment (relationships between latents match relationships between features), using adaptive weighting similar in spirit to loss balancing in GANs [^goodfellow2014generative]. This yields semantically organized high-dimensional latent spaces that retain reconstruction quality while being more learnable for downstream generative models.

By semantically structuring the latent space of the VAE, it reduces the diffusion model's burden, allowing it to focus on learning the distribution rather than also discovering semantic organization. The latent space provides appropriate inductive bias—semantic structure "baked in" through VFM alignment while pixel details are captured through reconstruction.

![VFM-VAE: Leveraging multiple VFM encodings as compressed latents](/assets/img/blog/r4g/VAVAE_to_VFMVAE.png)
*Fig 9. In VFM-VAE, multiple VFM encodings are compressed into a single latent representation that is then projected out to pixel space via multi-scale decoders. The right side shows that this latent space is more robust to geometric perturbations and achieves strong reconstruction as well as generation.*

After diffusion model alignment as well as VAE alignment had been demonstrated, REPA-E [^leng_repa-e_2025] takes integration further through joint VAE+diffusion training, challenging the convention that these components should train separately. It demonstrates that while naive end-to-end training with diffusion loss alone is ineffective (causing latent space collapse), representation alignment provides necessary constraints for successful joint optimization. The key innovation proved to be careful gradient control. Alignment loss flows to both VAE and diffusion model, but diffusion loss uses stop-gradient on the VAE encoder to prevent collapse (the VAE shouldn't change to make diffusion easier at reconstruction's cost). In addition, to keep the latent space normalised, the VAE receives alignment gradients only through BatchNorm normalisation. This enables joint optimization: the VAE improves to produce latents both reconstructive *and* well-aligned, while the diffusion model learns in this evolving but stable space. Joint optimization improves the VAE itself, leading to better latent structure (higher VFM alignment, better class separation) and downstream performance. While in LSGM [^vahdat_score-based_2021] pretraining of the VAE was necessary, true end-to-end training is now possible.

![REPA-E: End-to-end joint VAE+diffusion training](/assets/img/blog/r4g/REPA_to_REPAE.png)
*Fig 10. Left shows stage-wise training with frozen VAE. Right shows REPA-E with careful gradient flow: alignment loss flows to both components, diffusion loss uses stop-gradient on VAE encoder, and VAE receives alignment gradients through BatchNorm for latent normalization.*

3stage-aligner [^chen_aligning_2025] proposes an alternative strategy: rather than training the VAE from scratch with alignment, freeze a pretrained encoder (e.g., DINOv2), map into into a low-dimension space via an adapter block and learn to align a decoder through three stages. Stage 1 (Latent Alignment) freezes the VFM encoder and trains the adapter plus decoder, establishing a semantic latent space with basic reconstruction capabilities. The resulting latents are semantically grounded but exhibit color shifts and missing fine-grained details since the frozen encoder was not trained for reconstruction. Stage 2 (Perceptual Alignment) jointly optimizes adapter and encoder (now unfrozen) with semantic preservation loss maintaining alignment with original frozen VFM features:

$$
\mathcal{L}_{\text{Stage 2}} = \mathcal{L}_{\text{recon}} + \lambda_2 \lVert\text{Enc}(x) - \text{Enc}_{\text{frozen}}(x) \rVert^2
$$

The L2 loss prevents encoder drift from the pretrained semantic structure while allowing capture of fine-grained color and texture. Stage 3 (Decoder Refinement) freezes both encoder and adapter, allowing the decoder to better exploit the latent representation changed during Stage 2 without disturbing semantic structure.

![3stage-aligner: Three-stage frozen encoder alignment](/assets/img/blog/r4g/VFMVAE_to_3stagealigner.png)
*Fig 11. Stage 1 establishes semantic grounding with frozen encoder. Stage 2 allows encoder refinement with semantic preservation loss. Stage 3 optimizes decoder for reconstruction quality, carefully balancing semantic preservation with fine-grained detail capture.*

This yields semantically rich tokenizers where latent space inherits discriminative structure from the pretrained encoder. The three-stage process carefully balances semantic preservation (maintaining VFM structure) with reconstruction quality (capturing fine-grained details), avoiding color shifts of purely frozen encoders and semantic drift of fully unconstrained fine-tuning.

Phase 2 establishes that incorporating semantic structure directly into VAE latent space—whether through alignment loss during training (VA-VAE), end-to-end joint optimization (REPA-E), or staged adaptation of frozen encoders (3stage-aligner)—produces superior results compared to standard pixel-focused VAE training. These semantically-structured spaces are easier to learn (faster convergence) and produce better final quality. However, they still rely on the two-stage VAE+diffusion pipeline, raising a natural question: do we need VAE compression at all?

## Phase 3: Operating Directly in Vision Foundation Model Feature Spaces

The third phase represents a more radical departure, questioning whether the VAE bottleneck is necessary at all. Instead of compressing images through a VAE and then aligning the latent space, these methods propose directly using pretrained vision foundation model features as the "latent space" for diffusion, or training autoencoders specifically to preserve discriminative information rather than minimize reconstruction error.

![Phase 3 overview: Eliminating VAE compression](/assets/img/blog/r4g/diffuse_in_embedding_space.png)
*Fig 12. VFM-VAE uses frozen VFM features with multi-scale fusion and progressive reconstruction. SVG uses frozen DINO with residual encoder for fine-grained details. RAE replaces VAE entirely with pretrained representation encoders.*

Based on the observation made in Perception Encoder [^bolya_perception_2025] that the best visual embeddings for downstream tasks are often not at the output of vision networks but rather in intermediate layers, VFM-VAE [^bi_vision_2025] merges frozen VFM features from different parts of the network as latent representations. However, VFMs focus on semantic understanding, producing spatially coarse features (e.g., DINOv2 ViT-L outputs $16 \times 16$ for $256 \times 256$ images) sacrificing pixel fidelity. VFM-VAE redesigns the decoder with multi-scale latent fusion (combining features from multiple VFM layers, providing both semantic guidance from deep layers and spatial detail from shallow layers) and progressive resolution reconstruction (building up resolution gradually through decoder blocks, starting from coarse VFM features and progressively adding detail). In addition, the embedding dimensionality of VFMs is often too high for effective generative modelling; VFM-VAE circumvents this by mapping the different embeddings into a compressed latent space that is regularised via KL divergence, thereby still containing a VAE but with strong initialisation by a VFM.

This enables high-quality reconstruction from semantically rich but spatially compact representations. The work also introduces SE-CKNNA metric for diagnosing representation dynamics during diffusion training. SE-CKNNA measures how well semantic structure in latent space is preserved during noising, revealing that semantic structure degrades nonlinearly with noise level, with critical thresholds where class separability breaks down. Using these insights, the authors develop joint tokenizer-diffusion alignment strategy dramatically accelerating convergence. The frozen pretrained encoder ensures the latent space maintains semantic alignment even under distribution shifts—Phase 2 methods that fine-tune encoders risk semantic drift; VFM-VAE's frozen encoder ensures consistent structure. However, this requires architectural innovations (multi-scale fusion, progressive reconstruction) to overcome reconstruction challenges of coarse frozen features, which prevents easy adoption.

SVG [^shi_latent_2025] tries to avoid these complex architectural modifications by taking a principled approach analyzing why VAE latent spaces are problematic: they lack clear semantic separation and strong discriminative structure. Standard VAE latents exhibit semantic entanglement (different classes overlap) and poor class compactness (same-class samples widely dispersed). This makes the distribution difficult for diffusion to learn, as it must simultaneously discover semantic structure and model fine-grained variation. To overcome this, SVG constructs latent representations from frozen DINO features providing semantically discriminative structure with clear class separation, augmented with lightweight residual branch capturing fine-grained details:

$$
z_{\text{final}} = z_{\text{DINO}} + \alpha \cdot z_{\text{residual}}
$$

where frozen DINOv2 provides semantics and a learned residual encoder captures color, texture, and other details DINO discards. Normal VAE latents are semantically entangled, but alignment to VFM models enables clearer class separation and more compact classes. The SVG encoder proves important for fine-grained color details. No diffusion model tricks are needed since in the case of the chosen VFM DINOv3, the latent space is small enough (384-dimensional) to be modelled without compression. However, the alignment loss is crucial: without it, the decoder over-relies on the residual encoder, and numerical range differences between normalized frozen DINOv3 features and unnormalized learned residuals can distort semantic embeddings.

![SVG: Using frozen DINO with residual encoder](/assets/img/blog/r4g/VAVAE_to_SVG.png)
*Fig 13. Left shows VA-VAE with learned encoder aligned to VFM. Right shows SVG with frozen DINO encoder plus lightweight residual encoder capturing fine-grained details, enabling clearer semantic separation without VAE training.*

While SVG emphasises the need for a modest embedding space dimensionality and the need for a residual encoder that makes up for missing pixel-level details in the VFM embeddings, RAE [^zheng_diffusion_2025] tries to replace the VAE solely with pretrained representation encoders paired with trained decoders, without additional compression or auxiliary encoders. The authors systematically explore encoders from diverse self-supervised methods (DINO, SigLIP, MAE) and analyze challenges of operating diffusion transformers in resulting high-dimensional spaces. While standard VAE latents are low-dimensional ($32 \times 32 \times 4$, or 4K dimensions), representation encoder outputs are much higher ($16 \times 16 \times 1024$ for DINOv2 ViT-L, or 262K dimensions). This poses challenges for diffusion transformers that generally perform poorly in such high-dimensional spaces.

RAE identifies and addresses sources of difficulty through theoretically motivated solutions. First, standard DiT bottlenecks all tokens through the same hidden dimension, so when input tokens have higher dimensionality, this creates an information bottleneck. RAE introduces a wide DDT head that maintains high-dimensional representations through a final shallow-but-wide layer while keeping the majority of the DiT block lower-dimensional. Second, standard schedules are designed based on spatial dimensions assuming certain statistical properties. Representation encoder outputs have different characteristics (already normalized, different variance structure). Therefore, RAE makes the noise schedule depend on actual data statistics rather than assuming fixed properties. Third, since the decoder trains separately from the frozen encoder, mismatch can occur at inference—the diffusion model produces slightly imperfect samples, but the decoder was trained on clean representations. Following TarFlow [^zhai2024normalizing], RAE adds noise augmentation during decoder training for robustness to imperfect samples.

RAE demonstrated that high-quality reconstruction from frozen DINO encoders with strong representations is possible. Computational overhead is minimal since DiT cost depends mostly on sequence length, not token dimension (which the wide head addresses). The DiT adjustments are necessary: scaling width to token dimension, making noise schedule data-dependent instead of spatial-dependent, and using noise-augmented decoding due to discrete decoder training. An additional benefit of RAE is that high-resolution synthesis is trivially enabled by swapping decoders with different patch sizes—the frozen encoder and trained diffusion model remain unchanged.

![RAE: Comprehensive framework for representation autoencoders](/assets/img/blog/r4g/SVG_to_RAE.png)
*Fig 14. Shows systematic exploration of different pretrained encoders (DINO, SigLIP, MAE) as frozen latent encoders, with DiT adjustments (wide DDT head, data-dependent noise schedule, noise-augmented decoding) enabling effective diffusion in high-dimensional representation spaces.*

However, while RAE allowed the direct use of pretrained VFMs as encoders, it has two main limitations:
1. The modifications of the diffusion model required to make this work were substantial.
2. There is no emphasis whatsoever on reconstruction, limiting editing capabilities of these models and making them potentially vulnerable to drifting off the data manifold.

FAE [^gao_one_2025] focuses on tackling the first of these challenges by introducing a simple adoption via a single attention layer that allows the usage of standard LightningDiT recipes. By then training to both reconstruct images and preserve pretrained features, FAE creates truly unified representation serving as both generative latent space and discriminative feature space. The simple translation layer (a single attention layer between frozen encoder features and generative decoder) provides minimal but effective transformation. This allows use of standard diffusion models again without the RAE modifications, demonstrating that the right architectural intervention can eliminate the need for extensive model adjustments. It also shows that the simple translation layer preserves the spatial structure in latent space, which aligns with the iREPA insights that spatial structure is the main determinant for how effective alignment will be for generation quality [^singh_what_2025].

![FAE: Streamlined unification of feature and generative spaces](/assets/img/blog/r4g/RAE_to_FAE.png)
*Fig 15. Unlike RAE, FAE introduces a lightweight "translation layer" (a single attention block) to align frozen pretrained encoder features with the generative decoder. This minimal intervention preserves spatial structure and discriminative power.*

While FAE tackled the architectural adoption problem of RAE, PS-VAE [^zhang2025psvae] tackled the editing problem that comes with the fact that RAE does not encourage the latent space to encode reconstruction capability explicitly. By training sequential representation as well as pixel decoders as well as finetuning the pretrained representation encoder with reconstruction losses, they find a good balance between reconstruction and representation capabilities and show that this balance allows them to perform superior generation and editing.

Most recently, UAE [^fan2025harmonizing] offers a theoretical unification through its "Prism Hypothesis," which posits that semantic and pixel representations correspond to different frequency bands of a shared spectrum. Unlike SVG which adds a separate residual encoder, or RAE which relies on a heavy decoder, UAE initializes its encoder from DINOv2 and utilizes a frequency-band modulator to disentangle the latent space. It explicitly aligns the low-frequency band to the semantic teacher while dedicating high-frequency bands to residual details, effectively harmonizing semantic abstraction with pixel fidelity in a single compact latent space.

Phase 3 methods establish that VAE compression is not fundamental to high-quality latent diffusion. By directly using pretrained vision foundation model features as latent representations (with appropriate architectural modifications handling high-dimensionality, spatial coarseness, and reconstruction challenges), we achieve generation quality comparable to or exceeding VAE-based methods while maintaining discriminative power of the original pretrained encoder. However, all Phase 3 methods still rely on pretrained vision foundation models. Phase 4 takes the final step: questioning whether we need pretrained representations at all.

## Phase 4: Questioning the Need for Pretrained Representations

After three phases focused on progressively sophisticated ways to leverage pretrained models, Phase 4 represents a countertrend: can we achieve similar benefits by training from scratch with better objectives and architectures? This phase questions whether dependency on external pretrained models is fundamental or merely a workaround for suboptimal training procedures.

USP [^chu_usp_2025] embodies this philosophy through fully end-to-end training jointly optimized for both generative and discriminative objectives. Rather than initializing from external representations, it employs a multi-task loss combining generation and discrimination such as contrastive learning, masked prediction, or classification. Generative and discriminative objectives complement one another: generative learning encourages modeling the full data distribution, while discriminative tasks promote the discovery of semantically meaningful structure. Joint optimization thus produces representations that are simultaneously generative (capable of synthesis) and discriminative (useful downstream), reducing the reliance on separate pretraining stages. This raises a critical question: does representation alignment solve deep architectural deficiencies, or does it merely accelerate learning? If the latter, the necessity of pretrained models could wane as compute, data, and training recipes continue to scale.

A similar spirit underlies large-scale systems such as FLUX2-VAE [^noauthor_black_nodate], which demonstrates that sophisticated tokenizers can be learned directly through end-to-end training rather than depending on pretrained vision foundation features. Although little is publicly known about its technical details, FLUX2-VAE's production success suggests that with sufficient scale and engineering, high-quality tokenizers and representations can emerge organically from task training alone. Yet, "without pretrained representations" does not necessarily mean "cheap to train": the total computational cost may rival or even exceed that of conventional pretraining pipelines. Whether the elegance of end-to-end architectures outweighs the modularity, interpretability, and reusability of pretrained components remains an open question.

The same shift is visible in the recent renaissance of pixel-space diffusion models, which challenge the long-held assumption that latent diffusion is a prerequisite for high-resolution, high-quality generation. Methods such as JiT (Just image Transformer) [^li_back_2025], PixelDiT [^yu_pixeldit_2025], DeCo (frequency-DeCoupled diffusion) [^ma_deco_2025], DiP (Diffusion in Pixel space) [^chen_dip_2025], and SiD2 (Simpler Diffusion v2) [^hoogeboom_simpler_2025] illustrate a broader trend: architectural innovation can substitute for latent-space compression. By employing patch-based Transformers, efficient multi-scale attention, or frequency-aware loss designs, these models demonstrate that the efficiency, quality, and stability advantages traditionally attributed to latent spaces can also be achieved through direct pixel-space training. The result is a growing recognition that high-resolution generative performance depends less on where the model operates (latent or pixel space) and more on how it is structured and optimized.

EPG [^lei_advancing_2025] pushes this idea further by integrating representation learning into pixel-space diffusion itself. Rather than discarding the notion of learned structure, it reimagines representation pretraining as part of the diffusion process. EPG pretrains encoders through self-supervised objectives along deterministic diffusion trajectories, learning temporally consistent and semantically distinct features directly in pixel space. This pretraining endows the encoder with structured initialization analogous to pretrained vision models, but derived natively from the diffusion task. The result is a model that successfully trains consistency and diffusion systems from scratch, reportedly the first to achieve stable training of high-resolution consistency models without any pretrained VAEs or diffusion models.

Together, these innovations in pixel-space diffusion reinforce Phase 4's premise: high-quality representations and generative performance need not rely on external pretraining. As compute and data availability scale, architectural ingenuity and task-specific pretraining increasingly bridge the gap once filled by large pretrained backbones. However, fully end-to-end models remain computationally demanding and fragile during optimization. In contrast, pretrained components provide stability and reduce redundant computation across research efforts. Empirically, pretrained models still dominate in efficiency at moderate scales, while end-to-end approaches begin to overtake them only at extreme scales. In practice, the future may lean toward hybrid systems, i.e pretrained models for rapid development and exploration, and large-scale, task-specific end-to-end training for production models where the additional compute cost yields tangible performance gains.

## Generative Models as Representations

An alternative perspective on unifying generation and representation learning is to use generative modeling itself as the pretraining objective, treating the features learned during generative training as useful representations for downstream tasks.

MAE [^he2022masked] pioneered masked pixel reconstruction for vision, demonstrating that predicting masked image patches creates strong representations. However, the pixel-level reconstruction objective tends to focus on low-level details rather than high-level semantics. Could predicting embeddings instead of pixels yield better representations?

AIM v1 (Autoregressive Image Models) [^el2024scalable] revisits autoregressive modeling for vision with modern architectures and large-scale data. Unlike early work like iGPT [^chen2020generative] or D-iGPT [^ren2023rejuvenating], AIM uses Vision Transformers and is trained on billions of images. The work demonstrates two key findings: (1) visual feature performance scales with both model capacity and data quantity, exhibiting similar scaling laws to large language models, and (2) the value of the autoregressive objective function correlates with downstream performance, providing a meaningful training signal. AIM-7B achieves 84.0% ImageNet fine-tuning accuracy and shows particularly strong performance when trained on diverse, uncurated web data. The autoregressive objective—predicting the next patch given previous patches—naturally creates representations that capture sequential dependencies and can be more easily scaled than traditional contrastive or masked modeling approaches.

AIM v2 [^fini2025multimodal] extends this to multimodal autoregressive models, demonstrating that the same autoregressive paradigm can be applied across images and text, creating unified representations that span modalities.

NEPA (Next-Embedding Prediction) [^xu_next-embedding_2025] borrows the AIM idea of predicting embeddings from pretrained models rather than raw pixels or image patches. By operating in a semantic embedding space, NEPA focuses on high-level features rather than low-level details, potentially leading to more efficient and semantically meaningful representation learning. The authors suggest that this approach could be extended to operate in latent spaces, bridging generative objectives with the representation-focused methods discussed in earlier sections.

The broader pattern here is that generative objectives—whether autoregressive, masked, or diffusion-based—can serve dual purposes: they enable sampling of new examples and, as a byproduct, learn representations useful for discriminative tasks. Recent work on improving diffusion autoencoders [^skorokhodov_improving_2025] and using masked autoencoders as tokenizers [^chen_masked_2025] further blurs the line between generative and representation learning, suggesting these are complementary views of the same underlying learning process.

Additional relevant work includes methods that explicitly bridge generative and discriminative training. Robust representation consistency models [^lei_robust_2025] use contrastive denoising to learn consistent representations along diffusion trajectories, improving both robustness and downstream performance. The dispersive loss [^wang_diffuse_2025] provides a simple plug-and-play regularizer that encourages diffusion model representations to disperse in hidden space (analogous to contrastive learning) without requiring positive pairs, improving generation quality without interfering with the sampling process and was used in previously mentioned pixel-space diffusion models like EPG [^lei_advancing_2025].

## Representation Learning and Alignment in Molecular Machine Learning

The ideas from visual representation learning and generative modeling are beginning to influence molecular and protein modeling, suggesting broader applicability of these concepts beyond computer vision. In particular, there is a growing interest in molecular representation spaces that play a role analogous to DINO or SigLIP in vision, serving as generic, pretrained feature spaces for a wide range of downstream tasks [^bernstein2024gap][^li2025platonic].

MACE (Message Passing Atomic Cluster Expansion) has emerged as a foundation model for atomistic materials chemistry, combining equivariant message passing with systematically improvable many-body basis functions [^batatia2025foundation][^bernstein2024gap]. Trained on large collections of quantum-mechanical reference data, MACE learns rich local representations of atomic environments that generalize across diverse chemistries, phases, and thermodynamic conditions, thereby playing an analogous role to vision foundation models in the molecular domain [^batatia2025foundation]. Crucially, these representations seem to generalise well across molecular modalities, allowing not only accurate property predictions in materials (the original field of study) but also molecules [^wedig2025rem3di] and proteins [^bojan2025representing].

MACE-REPA directly applies the representation alignment paradigm to molecular force fields, mirroring Phase 1 methods developed for diffusion models in vision [^pinede2025unifying]. Instead of aligning diffusion features to DINO-like vision encoders, MACE-REPA aligns the intermediate representations learned during molecular dynamics force-field training with pretrained MACE features, using auxiliary losses that encourage consistency between the evolving force-field encoder and a frozen foundation potential. This demonstrates that the core insight of representation alignment—leveraging structured pretrained representations to accelerate and stabilize training—transfers robustly from image diffusion models to atomistic simulations.

Beyond a single model family, unified molecular feature spaces have been explored by multiple groups, echoing the Platonic Representation Hypothesis in vision. Work from MIT demonstrates that ostensibly different molecular models, trained on overlapping quantum-chemistry corpora, can be mapped into a common latent space with minimal loss in predictive performance, indicating that they encode closely related underlying chemical manifolds [^edamadaka2025universally]. Complementary work from London analyzes the geometry of feature spaces learned by NNPs such as MACE and related architectures, providing empirical evidence that independently trained models converge toward similar "platonic" representations of molecular structure and energetics [^li2025platonic]. Together, these results suggest that there are strong constraints on how molecular structure and forces can be represented, and that sufficiently expressive models trained on large, diverse datasets tend to discover comparable latent organizations, a similar trend as in computer vision.

In parallel, people also try to construct custom VAEs for representation learning in the biomolecular domain, similar to the FLUX-line of work in computer vision. One such example is SLAE [^chen2025slae]: rather than encoding entire molecules or proteins as global graphs, SLAE constructs latent representations at the level of strictly local all-atom environments (with a locality bias similar to MACE that could allow stronger generalisation), and trains them with a combination of reconstruction losses and auxiliary property objectives such as hydration energies, force-field terms, or stability proxies. This places SLAE conceptually close to recent image-space autoencoders that blend reconstruction and semantic objectives: reconstruction ensures geometric fidelity, while the auxiliary heads encourage the latent space to align with physically meaningful axes, thereby bridging representation and reconstruction in an analogous way to aligned VAEs and representation autoencoders in vision.

## Conclusion

The field has undergone rapid evolution, progressing through four distinct phases: (1) aligning diffusion features with pretrained representations, (2) incorporating semantic structure into VAE latent spaces, (3) directly using pretrained representations as latent spaces, and (4) questioning whether pretrained representations are necessary at all. Parallel developments in pixel-space diffusion and generative representation learning have further enriched the landscape.

Several clear patterns emerge: representation alignment dramatically accelerates training, spatial structure may be more important than global semantics, VAE compression is not fundamental, and principles transfer beyond vision to molecular modeling. However, fundamental questions remain: What makes representations learnable? What is the optimal compression rate? How do we unify multiple modalities? Should we train jointly or in stages?

The answers likely depend on scale, application requirements, and computational constraints. At research scale, leveraging pretrained models provides clear advantages. At production scale, end-to-end training may be preferable despite higher cost. For maximum quality, pixel-space methods avoid reconstruction artifacts. For maximum efficiency, latent-space methods reduce computation.

Looking forward, how these developments might influence the development of molecular machine learning is an interesting question that part of my research proposal will try to answer.

## References

[^ho2020ddpm]: Ho, J., et al. (2020). Denoising Diffusion Probabilistic Models. *NeurIPS*.
[^song2020score]: Song, Y., et al. (2020). Score-Based Generative Modeling through Stochastic Differential Equations. *ICLR*.
[^lai2025principles]: Lai, C.-H., et al. (2025). The principles of diffusion models. *arXiv*.
[^lipman2024flow]: Lipman, Y., et al. (2024). Flow Matching for Generative Modeling. *ICLR*.
[^albergo2023building]: Albergo, M. S., & Vanden-Eijnden, E. (2023). Building Normalizing Flows with Stochastic Interpolants. *ICLR*.
[^radford2021learning]: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. *ICML*.
[^caron_emerging_2021]: Caron, M., et al. (2021). Emerging Properties in Self-Supervised Vision Transformers. *ICCV*.
[^he2022masked]: He, K., et al. (2022). Masked Autoencoders Are Scalable Vision Learners. *CVPR*.
[^rombach_high-resolution_2022]: Rombach, R., et al. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR*.
[^blattmann2023align]: Blattmann, A., et al. (2023). Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. *CVPR*.
[^brooks2024sora]: Brooks, T., et al. (2024). Video generation models as world simulators. *OpenAI*.
[^podell2023sdxl]: Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. *ICLR*.
[^peebles2023scalable]: Peebles, W., & Xie, S. (2023). Scalable Diffusion Models with Transformers. *ICCV*.
[^esser2024sd3]: Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. *ICML*.
[^gao2024diffusion]: Gao, R., et al. (2024). Diffusion Models and Flow Matching are Equivalent. *arXiv*.
[^oord_representation_2019]: Oord, A. v. d., et al. (2019). Representation Learning with Contrastive Predictive Coding. *arXiv*.
[^chen_exploring_2020]: Chen, X., & He, K. (2020). Exploring Simple Siamese Representation Learning. *CVPR*.
[^caron_deep_2019]: Caron, M., et al. (2019). Deep Clustering for Unsupervised Learning of Visual Features. *ECCV*.
[^caron_unsupervised_2021]: Caron, M., et al. (2021). Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. *NeurIPS*.
[^assran_self-supervised_2023]: Assran, M., et al. (2023). Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. *CVPR*.
[^huh_platonic_2024]: Huh, M., et al. (2024). The Platonic Representation Hypothesis. *arXiv*.
[^chen2020simclr]: Chen, T., et al. (2020). A Simple Framework for Contrastive Learning of Visual Representations. *ICML*.
[^assran_v-jepa_2025]: Assran, M., et al. (2025). V-JEPA: Video Joint Embedding Predictive Architecture. *arXiv*.
[^oquab_dinov2_2024]: Oquab, M., et al. (2024). DINOv2: Learning Robust Visual Features without Supervision. *arXiv*.
[^simeoni_dinov3_2025]: Simeoni, O., et al. (2025). DINOv3. *arXiv*.
[^zhai_sigmoid_2023]: Zhai, X., et al. (2023). Sigmoid Loss for Language Image Pre-Training. *ICCV*.
[^tschannen_siglip_2025]: Tschannen, M., et al. (2025). SigLIP 2. *arXiv*.
[^bolya_perception_2025]: Bolya, D., et al. (2025). The Perception Encoder. *arXiv*.
[^grill_bootstrap_2020]: Grill, J.-B., et al. (2020). Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. *NeurIPS*.
[^richemond_byol_2020]: Richemond, P. H., et al. (2020). BYOL works even without batch statistics. *arXiv*.
[^skorokhodov_improving_2025]: Skorokhodov, I., et al. (2025). Improving Diffusion Autoencoders. *arXiv*.
[^chen_masked_2025]: Chen, Z., et al. (2025). Masked Autoencoders as Tokenizers. *arXiv*.
[^bi_vision_2025]: Bi, P., et al. (2025). VFM-VAE. *arXiv*.
[^zhou2021ibot]: Zhou, J., et al. (2021). iBOT: Image BERT Pre-Training with Online Tokenizer. *ICLR*.
[^batatia2025foundation]: Batatia, I., et al. (2025). A Foundation Model for Atomistic Materials Chemistry. *arXiv*.
[^yu_representation_2025]: Yu, S., et al. (2025). Representation Alignment for Generation. *arXiv*.
[^wu_representation_2025]: Wu, G., et al. (2025). Representation Entanglement for Generation. *arXiv*.
[^leng_repa-e_2025]: Leng, S., et al. (2025). REPA-E: End-to-End Joint VAE-Diffusion Training. *arXiv*.
[^yao_reconstruction_2025]: Yao, X., et al. (2025). VA-VAE: Aligning VAE Latents to VFM. *arXiv*.
[^wang_repa_2025]: Wang, Y., et al. (2025). HASTE: Early-Stopped Alignment. *arXiv*.
[^wang_diffuse_2025]: Wang, T., et al. (2025). Diffuse, Disperse, Distill. *arXiv*.
[^xiang_denoising_2023]: Xiang, W., et al. (2023). Denoising Diffusion Autoencoders are Unified Self-Supervised Learners. *ICCV*.
[^chen2024deconstructing]: Chen, H., et al. (2024). Deconstructing Denoising Diffusion Models for Self-Supervised Learning. *arXiv*.
[^singh_what_2025]: Singh, J., et al. (2025). What Matters in Representation Alignment? *arXiv*.
[^dieleman2025latents]: Dieleman, S., et al. (2025). On the alignment of latents. *arXiv*.
[^goodfellow2014generative]: Goodfellow, I., et al. (2014). Generative Adversarial Networks. *NeurIPS*.
[^chen_aligning_2025]: Chen, Z., et al. (2025). 3-Stage Aligner. *arXiv*.
[^shi_latent_2025]: Shi, R., et al. (2025). SVG: Semantic Vision-to-Generation. *arXiv*.
[^zheng_diffusion_2025]: Zheng, Z., et al. (2025). RAE: Representation AutoEncoder. *arXiv*.
[^zhai2024normalizing]: Zhai, S., et al. (2024). Normalizing Flows for Probabilistic Modeling. *arXiv*.
[^gao_one_2025]: Gao, S., et al. (2025). FAE: One-Layer Translation for Representation Alignment. *arXiv*.
[^zhang2025psvae]: Zhang, S. (2025). PS-VAE: Both Semantics and Reconstruction Matter. *arXiv*.
[^fan2025harmonizing]: Fan, W., et al. (2025). The Prism Hypothesis. *arXiv*.
[^chu_usp_2025]: Chu, X., et al. (2025). USP: Unified Self-Supervised Pretraining. *arXiv*.
[^noauthor_black_nodate]: Black Forest Labs. (n.d.). FLUX.1.
[^li_back_2025]: Li, T., & He, K. (2025). Back to Basics: Just Image Transformers (JiT). *arXiv*.
[^yu_pixeldit_2025]: Yu, Y., et al. (2025). PixelDiT: Pixel Diffusion Transformers. *arXiv*.
[^ma_deco_2025]: Ma, Z., et al. (2025). DeCo: Frequency-Decoupled Pixel Diffusion. *arXiv*.
[^chen_dip_2025]: Chen, Z., et al. (2025). DiP: Diffusion in Pixel Space. *arXiv*.
[^hoogeboomsimpler2025]: Hoogeboom, E., et al. (2025). SiD2: Simpler Diffusion v2. *arXiv*.
[^lei_advancing_2025]: Lei, J., et al. (2025). EPG: External Pretraining for Generation. *arXiv*.
[^el2024scalable]: El-Nouby, A., et al. (2024). Scalable Autoregressive Image Generation. *arXiv*.
[^chen2020generative]: Chen, M., et al. (2020). Generative Pretraining from Pixels. *ICML*.
[^ren2023rejuvenating]: Ren, S., et al. (2023). Rejuvenating Image-GPT. *arXiv*.
[^fini2025multimodal]: Fini, E., et al. (2025). Multimodal Autoregressive Pretraining. *arXiv*.
[^xu_next-embedding_2025]: Xu, Z., et al. (2025). Next-Embedding Prediction. *arXiv*.
[^lei_robust_2025]: Lei, J., et al. (2025). Robust Representation Consistency Model. *arXiv*.
[^bernstein2024gap]: Bernstein, N., et al. (2024). GAP and MACE. *arXiv*.
[^li2025platonic]: Li, Z., et al. (2025). Platonic representation of foundation machine learning interatomic potentials. *arXiv*.
[^wedig2025rem3di]: Wedig, T., et al. (2025). REM-3Di. *arXiv*.
[^bojan2025representing]: Bojan, P., et al. (2025). Representing Proteins. *arXiv*.
[^pinede2025unifying]: Pinede, L., et al. (2025). Unifying Force Prediction and Molecular Conformation Generation. *ICML Workshop*.
[^edamadaka2025universally]: Edamadaka, S., et al. (2025). Universally Converging Representations of Matter. *arXiv*.
[^chen2025slae]: Chen, Y., et al. (2025). SLAE: Strictly Local All-atom Environment. *bioRxiv*.
[^kadkhodaie_unconditional_2025]: Kadkhodaie, Z., et al. (2025). Unconditional diffusion models learn semantically meaningful representations. *arXiv*.
[^liang_how_2024]: Liang, W., et al. (2024). How well do diffusion models understand vision? *arXiv*.
[^jaini_intriguing_2024]: Jaini, P., et al. (2024). The intriguing properties of generative classifiers. *arXiv*.
[^hoogeboomsimple2023]: Hoogeboom, E., et al. (2023). Simple diffusion: End-to-end diffusion for high resolution images. *arXiv*.


